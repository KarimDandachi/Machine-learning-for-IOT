{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "opposed-material",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-10003a8505ff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlite\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtflite\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mautograph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbitwise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow/_api/v2/compat/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_sys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mv1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mv2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mforward_compatibility_horizon\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow/_api/v2/compat/v1/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mautograph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbitwise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow/_api/v2/compat/v1/compat/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_sys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mv1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mv2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mforward_compatibility_horizon\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow/_api/v2/compat/v1/compat/v1/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlinalg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlite\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlookup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/bigdatalab_cpu_202101/lib/python3.7/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/bigdatalab_cpu_202101/lib/python3.7/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/bigdatalab_cpu_202101/lib/python3.7/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/bigdatalab_cpu_202101/lib/python3.7/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/bigdatalab_cpu_202101/lib/python3.7/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mget_code\u001b[0;34m(self, fullname)\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/bigdatalab_cpu_202101/lib/python3.7/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mget_data\u001b[0;34m(self, path)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow.lite as tflite\n",
    "from tensorflow import keras\n",
    "import zlib\n",
    "import argparse\n",
    "import tensorflow_model_optimization as tfmot   \n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--version', type=str, required=True, help='version')\n",
    "args = parser.parse_args()\n",
    "\n",
    "seed = 42\n",
    "tf.random.set_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "zip_path = tf.keras.utils.get_file(\n",
    "    origin='https://storage.googleapis.com/tensorflow/tf-keras-datasets/jena_climate_2009_2016.csv.zip',\n",
    "    fname='jena_climate_2009_2016.csv.zip',\n",
    "    extract=True,\n",
    "    cache_dir='.', cache_subdir='data')\n",
    "csv_path, _ = os.path.splitext(zip_path)\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "column_indices = [2, 5]\n",
    "columns = df.columns[column_indices]\n",
    "data = df[columns].values.astype(np.float32)\n",
    "\n",
    "n = len(data)\n",
    "train_data = data[0:int(n*0.7)]\n",
    "val_data = data[int(n*0.7):int(n*0.9)]\n",
    "test_data = data[int(n*0.9):]\n",
    "\n",
    "mean = train_data.mean(axis=0)\n",
    "std = train_data.std(axis=0)\n",
    "\n",
    "print (mean.shape)\n",
    "\n",
    "input_width = 6\n",
    "if args.version == \"a\" :                \n",
    "    output_steps = 3\n",
    "if args.version == \"b\" :\n",
    "    output_steps = 9\n",
    "\n",
    "\n",
    "class WindowGenerator:\n",
    "    def __init__(self, input_width, output_steps, mean, std):\n",
    "        self.input_width = input_width\n",
    "        self.output_steps = output_steps\n",
    "        self.mean = tf.reshape(tf.convert_to_tensor(mean), [1, 1, 2])\n",
    "        self.std = tf.reshape(tf.convert_to_tensor(std), [1, 1, 2])\n",
    "\n",
    "\n",
    "    def split_window(self, features):\n",
    "        inputs = features[:, :self.input_width, :]        # for example if total window size = 9 input =  [:,:6 ,:] --> output [:,-3: , ] outpu_tstep = 3 \n",
    "        labels = features[:, -self.output_steps :, :]\n",
    "\n",
    "        inputs.set_shape([None, self.input_width, 2])\n",
    "        labels.set_shape([None, self.output_steps,2])\n",
    "\n",
    "        return inputs, labels\n",
    "\n",
    "    def normalize(self, features):\n",
    "        features = (features - self.mean) / (self.std + 1.e-6)\n",
    "\n",
    "        return features\n",
    "\n",
    "    def preprocess(self, features):\n",
    "        inputs, labels = self.split_window(features)\n",
    "        inputs = self.normalize(inputs)\n",
    "\n",
    "        return inputs, labels\n",
    "\n",
    "    def make_dataset(self, data, train):\n",
    "        ds = tf.keras.preprocessing.timeseries_dataset_from_array(\n",
    "                data=data,\n",
    "                targets=None,\n",
    "                sequence_length = input_width + self.output_steps,                       #### this change because now the total depends on the output widht\n",
    "                sequence_stride = 1,\n",
    "                batch_size = 32)\n",
    "        ds = ds.map(self.preprocess)\n",
    "        ds = ds.cache()\n",
    "        if train is True:\n",
    "            ds = ds.shuffle(100, reshuffle_each_iteration=True)\n",
    "\n",
    "        return ds\n",
    "\n",
    "generator = WindowGenerator(input_width, output_steps, mean, std)\n",
    "train_ds = generator.make_dataset(train_data, True)\n",
    "val_ds = generator.make_dataset(val_data, False)\n",
    "test_ds = generator.make_dataset(test_data, False)\n",
    "\n",
    "print(f\"data shape before split {data.shape}\")\n",
    "\n",
    "print(f\"train_data {train_data.shape}\")\n",
    "\n",
    "print(f\"val_data {val_data.shape}\")\n",
    "\n",
    "print(f\"test_data {test_data.shape}\")\n",
    "\n",
    "\n",
    "# # Exercise 1.7\n",
    "class MultiOutputMAE(tf.keras.metrics.Metric):\n",
    "    def __init__(self, name='mean_absolute_error', **kwargs):\n",
    "        super().__init__(name=name, **kwargs)\n",
    "        self.total = self.add_weight('total', initializer='zeros', shape=(2,))\n",
    "        self.count = self.add_weight('count', initializer='zeros')\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        error = tf.abs(y_pred- y_true)     \n",
    "        error = tf.reduce_mean(error, axis=[0,1])  # compute the mean for all samples in the batch for each feature (temp , hum) ==> output shape = (2,)\n",
    "        self.total.assign_add(error)\n",
    "        self.count.assign_add(1.)\n",
    "\n",
    "        return\n",
    "\n",
    "    def reset_states(self):\n",
    "        self.count.assign(tf.zeros_like(self.count))\n",
    "        self.total.assign(tf.zeros_like(self.total))\n",
    "\n",
    "    def result(self):\n",
    "        result = tf.math.divide_no_nan(self.total, self.count)\n",
    "\n",
    "        return result\n",
    "\n",
    "alpha = 0.03\n",
    "# alpha = 1\n",
    "sparsity = 0.9\n",
    "Structured = True\n",
    "if Structured == True :\n",
    "    model_version = f\"_V( {version}_alpha={alpha} )\"\n",
    "else :\n",
    "    model_version = f\"_V( {version}_Sparcity ={sparsity} )\"\n",
    "mymodel = 'mlp'+ model_version\n",
    "chk_path = f'./callback_{mymodel}_chkp/{mymodel}_chkp_best'     # path for saving the best model \n",
    "TFLITE = mymodel + \".tflite\"                                    # path for saving the best model after converted to TF.lite model \n",
    "\n",
    "print(mymodel)\n",
    "\n",
    "\n",
    "\n",
    "#################################### Build models with Structured Pruning via width Multiplier #####################################################\n",
    "def bulid_models_Structured (alpha = alpha , version = version , input_width = input_width ,output_steps = output_steps ,model_version = model_version  ) :\n",
    "    mlp = tf.keras.Sequential([\n",
    "            tf.keras.layers.Flatten(input_shape = (input_width,2) , name='Flatten'),\n",
    "            tf.keras.layers.Dense(int(128 *alpha), activation='relu' , name='Dense1'),\n",
    "            tf.keras.layers.Dense(int(128 *alpha), activation='relu' , name='Dense2'),\n",
    "            tf.keras.layers.Dense(units = 2*output_steps , name='Output_layes'), \n",
    "            tf.keras.layers.Reshape([output_steps, 2])\n",
    "        ])\n",
    "\n",
    "    ############################################################################\n",
    "    cnn = tf.keras.Sequential([\n",
    "            tf.keras.layers.Conv1D(input_shape = (input_width,2) , filters=int(64 *alpha), kernel_size=3, activation='relu'),\n",
    "            tf.keras.layers.Flatten(),\n",
    "            tf.keras.layers.Dense(units=int(64 *alpha), activation='relu'),\n",
    "            tf.keras.layers.Dense(units=2*output_steps), \n",
    "            tf.keras.layers.Reshape([output_steps, 2])\n",
    "        ])\n",
    "\n",
    "  \n",
    "\n",
    "    MODELS = {'mlp'+ model_version: mlp, 'cnn'+ model_version: cnn }\n",
    "    return MODELS \n",
    "\n",
    "####################################       Build models with UnStructured Pruning         #####################################################\n",
    "\n",
    "def bulid_models_UnStructured ( version = version , input_width = input_width ,output_steps = output_steps ,model_version = model_version  ) :\n",
    "    mlp = tf.keras.Sequential([\n",
    "            tf.keras.layers.Flatten(input_shape = (input_width,2) , name='Flatten'),\n",
    "            tf.keras.layers.Dense(128, activation='relu' , name='Dense1'),\n",
    "            tf.keras.layers.Dense(128, activation='relu' , name='Dense2'),\n",
    "            tf.keras.layers.Dense(units = 2*output_steps , name='Output_layes'), \n",
    "            tf.keras.layers.Reshape([output_steps, 2])\n",
    "        ])\n",
    "\n",
    "    ############################################################################\n",
    "    cnn = tf.keras.Sequential([\n",
    "            tf.keras.layers.Conv1D(input_shape = (input_width,2) , filters=64, kernel_size=3, activation='relu' , name= \"Conv1D-1\"),\n",
    "            tf.keras.layers.Flatten(name='Flatten'),\n",
    "            tf.keras.layers.Dense(units=64, activation='relu', name='Dense-1'),\n",
    "            tf.keras.layers.Dense(units=2*output_steps), \n",
    "            tf.keras.layers.Reshape([output_steps, 2])\n",
    "        ])\n",
    "\n",
    " \n",
    "\n",
    "    MODELS = {'mlp'+ model_version: mlp, 'cnn'+ model_version: cnn }\n",
    "    return MODELS \n",
    "\n",
    "if Structured == True :\n",
    "    MODELS = bulid_models_Structured()\n",
    "else :\n",
    "    MODELS = bulid_models_UnStructured()\n",
    "    \n",
    "def get_model(model = mymodel ):\n",
    "    model = MODELS[model]\n",
    "    loss =   tf.keras.losses.MeanSquaredError()                       #tf.keras.losses.MeanSquaredError()\n",
    "    optimizer = tf.keras.optimizers.Adam()\n",
    "    metrics = [MultiOutputMAE()]\n",
    "\n",
    "    # Training and optimizing\n",
    "\n",
    "    model.compile(loss = loss, optimizer = optimizer, metrics = metrics)\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "# Create TEMP_HUM_VAL  callback to print the MAE for Temperature and humidity in more interpetable format \n",
    "class TEMP_HUM_VAL(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs):\n",
    "        hum = logs[\"val_mean_absolute_error\"][1]\n",
    "        temp = logs[\"val_mean_absolute_error\"][0]\n",
    "        MAE = logs[\"val_mean_absolute_error\"]\n",
    "        print(f\"\\n Temp MAE = {temp:.3f}, Hum MAE = {hum:.3f}    \")\n",
    "        # return temp , hum\n",
    "\n",
    "\n",
    "mycallback = TEMP_HUM_VAL()\n",
    "\n",
    "\n",
    "# Create checkpoint callback to save the best model \n",
    "cp_callback = keras.callbacks.ModelCheckpoint(\n",
    "    f'./callback_{mymodel}_chkp/{mymodel}_chkp_best',\n",
    "    # './callback_test_chkp/chkp_best',\n",
    "    monitor='val_loss',\n",
    "    verbose=0, \n",
    "    save_best_only=True,\n",
    "    save_weights_only=False,\n",
    "    mode='auto',\n",
    "    save_freq='epoch')\n",
    "\n",
    "if Structured == True :\n",
    "    model = get_model(mymodel)\n",
    "    history = model.fit(train_ds, epochs=20,   validation_data=val_ds,callbacks=[mycallback ,cp_callback ])\n",
    "if Structured == False :\n",
    "# Create  Unstructiured Pruning Callback  to to apply pruning during Training and fitting the model\n",
    "    model = MODELS[mymodel]\n",
    "    # Define the sparsity scheduler\n",
    "    pruning_params = {'pruning_schedule':\n",
    "    tfmot.sparsity.keras.PolynomialDecay(\n",
    "    initial_sparsity=0.30,\n",
    "    final_sparsity=sparsity,\n",
    "    begin_step=len(train_ds)*5,\n",
    "    end_step=len(train_ds)*15)\n",
    "    }\n",
    "\n",
    "    prune_low_magnitude = tfmot.sparsity.keras.prune_low_magnitude\n",
    "\n",
    "    model = prune_low_magnitude(model, **pruning_params)\n",
    "    # Define the pruning callback\n",
    "    PruningCallback = [tfmot.sparsity.keras.UpdatePruningStep()]\n",
    "    \n",
    "if Structured == False :\n",
    "    loss =   tf.keras.losses.MeanSquaredError()                       \n",
    "    optimizer = tf.keras.optimizers.Adam()\n",
    "    metrics = [MultiOutputMAE()]\n",
    "    input_shape = [32, 6, 2]\n",
    "    model.build(input_shape)\n",
    "    model.compile(loss = loss, optimizer = optimizer, metrics = metrics)\n",
    "    history = model.fit(train_ds, epochs=50,   validation_data=val_ds,callbacks=[ PruningCallback])\n",
    "\n",
    "    model_to_export = tfmot.sparsity.keras.strip_pruning(model)\n",
    "\n",
    "    \n",
    "############################## Print Model Summary ####################\n",
    "print(model.summary())\n",
    "\n",
    "\n",
    "def Pruned_Model_evaluate_and_compress_to_TFlite(model = model , model_to_export=None,tflite_model_dir =  TFLITE , model_name = mymodel  ):\n",
    "    \n",
    "    if not os.path.exists('./Pruned_models'):\n",
    "        os.makedirs('./Pruned_models')\n",
    "    # Model Evaluation\n",
    "    loss, error = model.evaluate(test_ds)\n",
    "    print(f'Temp mae = {error[0]:.3f}: , HUM mae = {error[1]:.3f} ')  \n",
    "    # Convert to TF lite without Quantization \n",
    "    converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "    tflite_model = converter.convert()  \n",
    "    # Write the model in binary formate and save it \n",
    "    with open(tflite_model_dir, 'wb') as fp:\n",
    "        fp.write(tflite_model)\n",
    "    Compressed =  f\"compressed_{TFLITE}\"\n",
    "    with open(Compressed, 'wb') as fp:\n",
    "        tflite_compressed = zlib.compress(tflite_model)\n",
    "        fp.write(tflite_compressed)\n",
    "    print(f\"the model is saved successfuly to {tflite_model_dir}\")\n",
    "    return Compressed , tflite_model_dir \n",
    "\n",
    "def apply_Quantization(tflite_model_dir =  TFLITE ,  PQT = False , WAPQT = False , Structured = Structured , saving_path = None ): \n",
    "    if Structured == False :\n",
    "        converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "    if Structured == True :\n",
    "        converter = tf.lite.TFLiteConverter.from_saved_model(saving_path)\n",
    "    # Apply weight only quantization \n",
    "    if PQT == True :\n",
    "        tflite_model_dir = f\"PQT_{tflite_model_dir}\"\n",
    "        converter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_SIZE]\n",
    "        tflite_model = converter.convert()\n",
    "    # Apply weight + Activation  quantization \n",
    "    if WAPQT == True :\n",
    "        converter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_SIZE]\n",
    "        converter.representative_dataset = representative_dataset_gen\n",
    "        tflite_model = converter.convert()\n",
    "        \n",
    "        tflite_model_dir = f\"WAPQT_{tflite_model_dir}\"\n",
    "      \n",
    "    # Write the model in binary formate and save it \n",
    "    with open(tflite_model_dir, 'wb') as fp:\n",
    "        fp.write(tflite_model)\n",
    "    Compressed =  f\"compressed_{tflite_model_dir}\"\n",
    "    with open(Compressed, 'wb') as fp:\n",
    "        tflite_compressed = zlib.compress(tflite_model)\n",
    "        fp.write(tflite_compressed)\n",
    "    print(f\"the model is saved successfuly to {tflite_model_dir}\")\n",
    "    return Compressed , tflite_model_dir \n",
    "\n",
    "# Function for weight and activations quantization \n",
    "def representative_dataset_gen():\n",
    "    for x, _ in train_ds.take(1000):\n",
    "        yield [x]\n",
    "\n",
    "def S_pruning_Model_evaluate_and_compress_to_TFlite( tflite_model_dir =  TFLITE , chk_path = chk_path , model_name = mymodel , PQT = False , WAPQT = False , Structured = Structured):\n",
    "    if not os.path.exists('./models'):\n",
    "        os.makedirs('./models')\n",
    "    model = tf.keras.models.load_model(filepath = chk_path , custom_objects={'MultiOutputMAE':MultiOutputMAE})\n",
    "\n",
    "    run_model = tf.function(lambda x: model(x))\n",
    "    # input_shape = model.inputs[0].shape.as_list()\n",
    "    # input_shape[0] = batch_size\n",
    "    # func = tf.function(model).get_concrete_function(\n",
    "    # tf.TensorSpec(input_shape, model.inputs[0].dtype))\n",
    "    concrete_func = run_model.get_concrete_function(tf.TensorSpec([1, 6, 2], tf.float32))\n",
    "    saving_path = os.path.join('.','models', model_name)\n",
    "    model.save(saving_path ,signatures=concrete_func )\n",
    "\n",
    "    best_model = tf.keras.models.load_model(filepath = saving_path , custom_objects={'MultiOutputMAE':MultiOutputMAE})\n",
    "    loss, error = best_model.evaluate(test_ds)\n",
    "    print(f'Temp mae = {error[0]:.3f}: , HUM mae = {error[1]:.3f} ')\n",
    "    # Convert to TF lite without Quantization \n",
    "    converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "    tflite_model = converter.convert()  \n",
    "\n",
    "    # Write the model in binary formate and save it \n",
    "    with open(tflite_model_dir, 'wb') as fp:\n",
    "        fp.write(tflite_model)\n",
    "    Compressed = \"compressed_\"+tflite_model_dir \n",
    "    with open(Compressed, 'wb') as fp:\n",
    "        tflite_compressed = zlib.compress(tflite_model)\n",
    "        fp.write(tflite_compressed)\n",
    "    print(f\"the model is saved successfuly to {tflite_model_dir}\")\n",
    "    return Compressed , tflite_model_dir , saving_path\n",
    "\n",
    "def load_and_evaluation(path, dataset):\n",
    "    interpreter = tf.lite.Interpreter(model_path = path) \n",
    "    interpreter.allocate_tensors()\n",
    "    input_details = interpreter.get_input_details()\n",
    "    output_details = interpreter.get_output_details()\n",
    "\n",
    "    dataset = test_ds.unbatch().batch(1)\n",
    "\n",
    "    outputs = []\n",
    "    labels = []\n",
    "    print(dataset)\n",
    "\n",
    "    for data in dataset:\n",
    "        my_input = np.array(data[0], dtype = np.float32)\n",
    "        label = np.array(data[1], dtype = np.float32)\n",
    "        # print (f\"my_input = {my_input}\")\n",
    "        # print(f\"label = {label}\")\n",
    "\n",
    "    \n",
    "            \n",
    "        labels.append(label)\n",
    "\n",
    "        interpreter.set_tensor(input_details[0]['index'], my_input)\n",
    "        interpreter.invoke()\n",
    "        my_output = interpreter.get_tensor(output_details[0]['index'])\n",
    "        \n",
    "        outputs.append(my_output[0])\n",
    "\n",
    "    outputs = np.squeeze( np.array(outputs))\n",
    "    labels = np.squeeze(np.array(labels))\n",
    "\n",
    "    \n",
    "    error = np.absolute(outputs - labels)\n",
    "  \n",
    "    mean_axis_1 = np.mean(error , axis = 1)     #  ==>  np.sum(error, axis = 1)/labels.shape[1]\n",
    "    \n",
    "    mae = np.mean(mean_axis_1 , axis = 0)  #  ==> np.sum(mean_axis_1, axis = 0) /mean_axis_1.shape[0]\n",
    "    temp_MAE = mae[0] \n",
    "    hum_MAE = mae[1]\n",
    "    print(\"*\"*50,\"\\n\",f\" Excuting the model {path} \")\n",
    "    print(\"*\"*50,\"\\n\",f\" mae is {mae} of shape {mae.shape} \")\n",
    "    print(\"*\"*50,\"\\n\",f'Temp mae = {mae[0]:.3f}: , HUM mae = {mae[1]:.3f} ')\n",
    "    \n",
    "    # check version \"a\" requirment\n",
    "    if version == \"a\" :\n",
    "        if temp_MAE <= 0.3 and hum_MAE <= 1.2 :\n",
    "            print (\"*\"*50,\"\\n\",\"achieved the requirments \" )\n",
    "        else :\n",
    "            print (\"*\"*50,\"\\n\",\"Not achieved the requirments\" )\n",
    "    # check version \"b\" requirment        \n",
    "    if version == \"b\" :\n",
    "        if temp_MAE <= 0.7 and hum_MAE <= 2.5 :\n",
    "            print (\"*\"*50,\"\\n\",\"achieved the requirments \" )\n",
    "        else :\n",
    "            print (\"*\"*50,\"\\n\",\"Not achieved the requirments\" ) \n",
    "\n",
    "\n",
    "def getsize(file):\n",
    "    st = os.stat(file)\n",
    "    size = st.st_size\n",
    "    return size\n",
    "\n",
    "if Structured == False :\n",
    "    tflite_model_dir_Compressed ,tf_lite_model_path = Pruned_Model_evaluate_and_compress_to_TFlite(model_to_export=model_to_export)\n",
    "if Structured == True :\n",
    "    tflite_model_dir_Compressed ,tf_lite_model_path, saving_path  = S_pruning_Model_evaluate_and_compress_to_TFlite(TFLITE)\n",
    "# print(b)\n",
    "tflite_size = getsize(tf_lite_model_path)\n",
    "compressed_tflite_size = getsize(tflite_model_dir_Compressed )\n",
    "print(f\" \\n Size of TF.lite model is {tflite_size /1000} KB\")\n",
    "print(f\" \\n Size of compressed_tflite model is {compressed_tflite_size /1000} KB\")\n",
    "print(tf_lite_model_path)\n",
    "load_and_evaluation(tf_lite_model_path , test_ds) \n",
    "if Structured == False :\n",
    "    tflite_model_dir_Compressed ,tf_lite_model_path = apply_Quantization( PQT=True)\n",
    "if Structured == True :\n",
    "    Compressed , Quantized   = apply_Quantization(PQT=True ,  saving_path=saving_path)\n",
    "# print(b)\n",
    "tflite_size = getsize(Quantized)\n",
    "compressed_tflite_size = getsize(Compressed )\n",
    "print(f\" \\n Size of TF.lite model is {tflite_size /1000} KB\")\n",
    "print(f\" \\n Size of compressed_tflite model is {compressed_tflite_size /1000} KB\")\n",
    "load_and_evaluation(Quantized , test_ds) \n",
    "\n",
    "if Structured == False :\n",
    "    tflite_model_dir_Compressed ,tf_lite_model_path = apply_Quantization( WAPQT=True)\n",
    "if Structured == True :\n",
    "    Compressed , Quantized   = apply_Quantization(WAPQT=True , saving_path=chk_path)\n",
    "# print(b)\n",
    "tflite_size = getsize(Quantized)\n",
    "compressed_tflite_size = getsize(Compressed )\n",
    "print(f\" \\n Size of TF.lite model is {tflite_size /1000} KB\")\n",
    "print(f\" \\n Size of compressed_tflite model is {compressed_tflite_size /1000} KB\")\n",
    "load_and_evaluation(Quantized , test_ds) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "foster-mineral",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
