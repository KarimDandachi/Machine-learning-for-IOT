{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7de5f391-271c-4f0a-83fe-3f5e54adf313",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version used to excute the code is 3.7.11\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow.lite as tflite\n",
    "from tensorflow import keras\n",
    "import zlib\n",
    "from platform import python_version\n",
    "import tensorflow_model_optimization as tfmot   \n",
    "import tempfile\n",
    "print(f\"Python version used to excute the code is {python_version()}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "953331be-1793-476e-a7e7-2fe647c2b40b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ######################################################## Input Parameters #########################################################\n",
    "# parser = argparse.ArgumentParser()\n",
    "# parser.add_argument('--model', type=str, required=True, help='model name')\n",
    "# parser.add_argument('--mfcc', action='store_true', help='use MFCCs')\n",
    "# args = parser.parse_args()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558050f7-e1c5-464b-9511-d95b13233d74",
   "metadata": {},
   "source": [
    "######################################################## Inputs and model selection  #########################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54d498d3-8121-4066-b58f-f668c83c95aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "version = \"a\"\n",
    "m = \"cnn\"   # model name [ mlp , cnn , ds_cnn  ]\n",
    "mfcc = True    # True --> excute mfcc , False --> excute STFT\n",
    "alpha = 0.3    # The width multiplier used to apply the structured Pruning \n",
    "\n",
    "model_version = f\"_V_{version}_alpha={alpha}\"\n",
    "\n",
    "mymodel = m + model_version\n",
    "TFLITE =  f'{mymodel}.tflite'                                   # path for saving the best model after converted to TF.lite model \n",
    "units = 8                                                       # The number of output class [8:without silence , 9 : with silence]\n",
    "################## Fix the Random seed to reproduce the same results \n",
    "seed = 42\n",
    "tf.random.set_seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f0986c75-d779-43a5-bc27-38e91607ff75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['stop' 'up' 'yes' 'right' 'left' 'no' 'down' 'go']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "zip_path = tf.keras.utils.get_file(\n",
    "    origin=\"http://storage.googleapis.com/download.tensorflow.org/data/mini_speech_commands.zip\",\n",
    "    fname='mini_speech_commands.zip',\n",
    "    extract=True,\n",
    "    cache_dir='.', cache_subdir='data')\n",
    "\n",
    "data_dir = os.path.join('.', 'data', 'mini_speech_commands')\n",
    "\n",
    "filenames = tf.io.gfile.glob(str(data_dir) + '/*/*')\n",
    "filenames = tf.random.shuffle(filenames)\n",
    "num_samples = len(filenames)\n",
    "\n",
    "total = 8000          # NUMBER OF TOTAL Files Including THE Silence records \n",
    "\n",
    "train_files =   filenames[: int(total*0.8)]                                             # filenames[:int(total*0.8)]\n",
    "val_files = filenames[int(total*0.8): int(total*0.9)]\n",
    "test_files = filenames[int(total*0.9):]\n",
    "\n",
    "# with silence ['stop', 'up', 'yes', 'right', 'left', 'no', 'silence', 'down', 'go']\n",
    "LABELS = np.array(['stop', 'up', 'yes', 'right', 'left', 'no',  'down', 'go'] , dtype = str) \n",
    "print (f\"The LABELS order as provided to the model are {LABELS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b68ce9-9b69-4ac6-b20f-73b4165d73e2",
   "metadata": {},
   "source": [
    "######################################################## Create the SignalGenerator #########################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e3ba740-f03b-4560-989b-9d12226806ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SignalGenerator:\n",
    "    def __init__(self, labels, sampling_rate, frame_length, frame_step,\n",
    "            num_mel_bins=None, lower_frequency=None, upper_frequency=None,\n",
    "            num_coefficients=None, mfcc=False):\n",
    "        self.labels = labels\n",
    "        self.sampling_rate = sampling_rate                                             # 16000  \n",
    "        self.frame_length = frame_length                                               # 640 \n",
    "        self.frame_step = frame_step                                                   # 320 \n",
    "        self.num_mel_bins = num_mel_bins                                               # 40 \n",
    "        self.lower_frequency = lower_frequency                                         # 20 \n",
    "        self.upper_frequency = upper_frequency                                         # 4000\n",
    "        self.num_coefficients = num_coefficients                                       # 10 \n",
    "        num_spectrogram_bins = (frame_length) // 2 + 1                                  # ( frame size // 2 ) + 1 \n",
    "\n",
    "        '''\n",
    "        STFT_OPTIONS = {'frame_length': 256, 'frame_step': 128, 'mfcc': False}\n",
    "        MFCC_OPTIONS = {'frame_length': 640, 'frame_step': 320, 'mfcc': True,\n",
    "        'lower_frequency': 20, 'upper_frequency': 4000, 'num_mel_bins': 40,\n",
    "        'num_coefficients': 10}\n",
    "        '''\n",
    "\n",
    "        if mfcc is True:                                                                # Remember we need to compute this matrix once so it will be a class argument \n",
    "            self.linear_to_mel_weight_matrix = tf.signal.linear_to_mel_weight_matrix(\n",
    "                    self.num_mel_bins, num_spectrogram_bins, self.sampling_rate,\n",
    "                    self.lower_frequency, self.upper_frequency)\n",
    "            self.preprocess = self.preprocess_with_mfcc\n",
    "        else:\n",
    "            self.preprocess = self.preprocess_with_stft\n",
    "\n",
    "    def read(self, file_path):\n",
    "        parts = tf.strings.split(file_path, os.path.sep)\n",
    "        label = parts[-2]                                  # -1 is audio.wav so \n",
    "        label_id = tf.argmax(label == self.labels)\n",
    "        audio_binary = tf.io.read_file(file_path)\n",
    "        audio, _ = tf.audio.decode_wav(audio_binary)\n",
    "        audio = tf.squeeze(audio, axis=1)\n",
    "\n",
    "        return audio, label_id\n",
    "\n",
    "    def pad(self, audio):\n",
    "        # Padding for files with less than 16000 samples\n",
    "        zero_padding = tf.zeros([self.sampling_rate] - tf.shape(audio), dtype=tf.float32)     # if the shape of the audio is already = 16000 (sampling rate) we will add nothing \n",
    "\n",
    "        # Concatenate audio with padding so that all audio clips will be of the  same length\n",
    "        audio = tf.concat([audio, zero_padding], 0)\n",
    "        # Unify the shape to the sampling frequency (16000 , )\n",
    "        audio.set_shape([self.sampling_rate])\n",
    "\n",
    "        return audio\n",
    "\n",
    "    def get_spectrogram(self, audio):\n",
    "        stft = tf.signal.stft(audio, frame_length=self.frame_length,\n",
    "                frame_step=self.frame_step, fft_length=self.frame_length)\n",
    "        spectrogram = tf.abs(stft)\n",
    "\n",
    "        return spectrogram\n",
    "\n",
    "    def get_mfccs(self, spectrogram):\n",
    "        mel_spectrogram = tf.tensordot(spectrogram,\n",
    "                self.linear_to_mel_weight_matrix, 1)\n",
    "        log_mel_spectrogram = tf.math.log(mel_spectrogram + 1.e-6)\n",
    "        mfccs = tf.signal.mfccs_from_log_mel_spectrograms(log_mel_spectrogram)\n",
    "        mfccs = mfccs[..., :self.num_coefficients]\n",
    "\n",
    "        return mfccs\n",
    "\n",
    "    def preprocess_with_stft(self, file_path):\n",
    "        audio, label = self.read(file_path)\n",
    "        audio = self.pad(audio)\n",
    "        spectrogram = self.get_spectrogram(audio)\n",
    "        spectrogram = tf.expand_dims(spectrogram, -1)                         # expand_dims will not add or reduce elements in a tensor, it just changes the shape by adding 1 to dimensions for the batchs. \n",
    "    \n",
    "        spectrogram = tf.image.resize(spectrogram, [32, 32])\n",
    "\n",
    "        return spectrogram, label\n",
    "\n",
    "    def preprocess_with_mfcc(self, file_path):\n",
    "        audio, label = self.read(file_path)\n",
    "        audio = self.pad(audio)\n",
    "        spectrogram = self.get_spectrogram(audio)\n",
    "        mfccs = self.get_mfccs(spectrogram)\n",
    "        mfccs = tf.expand_dims(mfccs, -1)\n",
    "\n",
    "        return mfccs, label\n",
    "\n",
    "    def make_dataset(self, files, train):\n",
    "        ds = tf.data.Dataset.from_tensor_slices(files)\n",
    "        ds = ds.map(self.preprocess, num_parallel_calls = tf.data.experimental.AUTOTUNE) # better than 4 tf.data.experimental.AUTOTUNE will use the maximum num_parallel_calls \n",
    "        ds = ds.batch(32)\n",
    "        ds = ds.cache()\n",
    "        ds = ds.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "        if train is True:\n",
    "            ds = ds.shuffle(100, reshuffle_each_iteration=True)\n",
    "\n",
    "        return ds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "145319b8-9d5d-4703-848d-286ef9d32308",
   "metadata": {},
   "source": [
    "######################################################## Options for MFCC & STFT #########################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0eceb5a7-dd9e-4016-8740-8a369cc04c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "STFT_OPTIONS = {'frame_length': 256, 'frame_step': 128, 'mfcc': False}\n",
    "MFCC_OPTIONS = {'frame_length': 640, 'frame_step': 320, 'mfcc': True,\n",
    "        'lower_frequency': 20, 'upper_frequency': 4000, 'num_mel_bins': 40,\n",
    "        'num_coefficients': 10}\n",
    "if mfcc is True:\n",
    "    options = MFCC_OPTIONS\n",
    "    strides = [2, 1]\n",
    "else:\n",
    "    options = STFT_OPTIONS\n",
    "    strides = [2, 2]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "960e663d-1878-4adc-9896-3b14f301c1a3",
   "metadata": {},
   "source": [
    "######################################################## Generate Data set splits #########################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d60c9552-9801-40fc-912f-68a80e4f7095",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = SignalGenerator(LABELS, 16000, **options)\n",
    "train_ds = generator.make_dataset(train_files, True)\n",
    "val_ds = generator.make_dataset(val_files, False)\n",
    "test_ds = generator.make_dataset(test_files, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5998873-661e-40e5-8700-126e7a7b776d",
   "metadata": {},
   "outputs": [],
   "source": [
    "############## checking shapes and values of data sets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "84c96043-735d-4894-b781-9850efd9a705",
   "metadata": {},
   "outputs": [],
   "source": [
    "# it = iter(val_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b4d86289-08a7-4575-bca6-c453c64833dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 49, 10, 1)\n",
      "tf.Tensor(0, shape=(), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "# inp , label = next(it)\n",
    "# print(inp.shape)\n",
    "# print(label[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ebef89-bc88-4cec-ab48-fa1c20b6255f",
   "metadata": {},
   "source": [
    "########################################################  building the models ########################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8b781a4b-06e1-456d-aee0-996e6a540e05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['mlp_V_a_alpha=0.3', 'cnn_V_a_alpha=0.3', 'ds_cnn_V_a_alpha=0.3'])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "mlp = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(units = int(256 *alpha), activation='relu' , name =  \"Dense-1\" ),\n",
    "    tf.keras.layers.Dense(units = int(256 *alpha), activation='relu', name =  \"Dense-2\"),\n",
    "    tf.keras.layers.Dense(units = int(256 *alpha), activation='relu', name =   \"Dense-3\" ),\n",
    "    tf.keras.layers.Dense(units = units , name =  \"Output-Layer\")                                   # change to 9 if silence included \n",
    "])\n",
    "\n",
    "cnn = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv2D(filters=int(128 *alpha), kernel_size=[3,3], strides=strides, use_bias=False , name = \"Conv2D-1\"),\n",
    "    tf.keras.layers.BatchNormalization(momentum=0.1 , name = \"Btch_Norm-1\"),\n",
    "    tf.keras.layers.ReLU(),\n",
    "    tf.keras.layers.Conv2D(filters=int(128 *alpha), kernel_size=[3,3], strides=[1,1], use_bias=False , name = \"Conv2D-2\"),\n",
    "    tf.keras.layers.BatchNormalization(momentum=0.1 , name = \"Btch_Norm-2\"),\n",
    "    tf.keras.layers.ReLU(),\n",
    "    tf.keras.layers.Conv2D(filters=int(128 *alpha), kernel_size=[3,3], strides=[1,1], use_bias=False , name = \"Conv2D-3\"),\n",
    "    tf.keras.layers.BatchNormalization(momentum=0.1 , name = \"Btch_Norm-3\"),\n",
    "    tf.keras.layers.ReLU(),\n",
    "    tf.keras.layers.GlobalAveragePooling2D( name =  \"GlobalAveragePooling-Layer\"),\n",
    "    tf.keras.layers.Dense(units = units, name =  \"Output-Layer\")\n",
    "])\n",
    "\n",
    "ds_cnn = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv2D(filters=int(256 *alpha), kernel_size=[3,3], strides=strides, use_bias=False, name = \"Conv2D-1\"),\n",
    "    tf.keras.layers.BatchNormalization(momentum=0.1),\n",
    "    tf.keras.layers.ReLU(),\n",
    "    tf.keras.layers.DepthwiseConv2D(kernel_size=[3, 3], strides=[1, 1], use_bias=False, name = \"DepthwiseConv2D-1\"),\n",
    "    tf.keras.layers.Conv2D(filters=int(256 *alpha), kernel_size=[1,1], strides=[1,1], use_bias=False, name = \"Conv2D-2\"),\n",
    "    tf.keras.layers.BatchNormalization(momentum=0.1),\n",
    "    tf.keras.layers.ReLU(),\n",
    "    tf.keras.layers.DepthwiseConv2D(kernel_size=[3, 3], strides=[1, 1], use_bias=False, name = \"DepthwiseConv2D-2\"),\n",
    "    tf.keras.layers.Conv2D(filters=int(256 *alpha), kernel_size=[1,1], strides=[1,1], use_bias=False, name = \"Conv2D-3\"),\n",
    "    tf.keras.layers.BatchNormalization(momentum=0.1),\n",
    "    tf.keras.layers.ReLU(),\n",
    "    tf.keras.layers.GlobalAveragePooling2D( name =  \"GlobalAveragePooling-Layer\"),\n",
    "    tf.keras.layers.Dense(units = units, name =  \"Output-Layer\")\n",
    "])\n",
    "\n",
    "\n",
    "MODELS = {'mlp'+ model_version : mlp, 'cnn'+ model_version: cnn, 'ds_cnn'+ model_version: ds_cnn}\n",
    "print(MODELS.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b273a830-946b-4b6f-8a93-9c67b908a50b",
   "metadata": {},
   "source": [
    "######################################################## Define optimizer & Losses & Metrics ########################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "61f4f328-b703-4a2c-952b-360d798f0b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = MODELS[mymodel]              # initiate the selected model \n",
    "\n",
    "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "optimizer = tf.optimizers.Adam()\n",
    "metrics = [tf.keras.metrics.SparseCategoricalAccuracy()]\n",
    "\n",
    "\n",
    "################### Compiling the model :\n",
    "\n",
    "model.compile(loss = loss, optimizer = optimizer, metrics = metrics)\n",
    "\n",
    "######################################################## check points depending on preprocessing STFT , MFCC \n",
    "if mfcc is False:\n",
    "    checkpoint_filepath = f'./checkpoints/stft/chkp_best_{mymodel}'\n",
    "\n",
    "else:\n",
    "    checkpoint_filepath = f'./checkpoints/mfcc/chkp_best_{mymodel}'\n",
    "    \n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,           \n",
    "    monitor='val_sparse_categorical_accuracy',\n",
    "    verbose=1,\n",
    "    mode='max',\n",
    "    save_best_only=True,\n",
    "    save_freq='epoch')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d26ff580-89c6-48a3-96d4-1512f83c51fc",
   "metadata": {},
   "source": [
    "######################################################## Model Training ########################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0ca7e4bd-a492-46a1-88be-1698e12e1fb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "197/200 [============================>.] - ETA: 0s - loss: 1.7126 - sparse_categorical_accuracy: 0.4616\n",
      "Epoch 00001: val_sparse_categorical_accuracy improved from -inf to 0.70250, saving model to ./checkpoints/mfcc\\chkp_best_cnn_V_a_alpha=0.3\n",
      "WARNING:tensorflow:From C:\\Users\\musta\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "WARNING:tensorflow:From C:\\Users\\musta\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "INFO:tensorflow:Assets written to: ./checkpoints/mfcc\\chkp_best_cnn_V_a_alpha=0.3\\assets\n",
      "200/200 [==============================] - 11s 53ms/step - loss: 1.7068 - sparse_categorical_accuracy: 0.4652 - val_loss: 1.2849 - val_sparse_categorical_accuracy: 0.7025\n",
      "Epoch 2/20\n",
      "200/200 [==============================] - ETA: 0s - loss: 1.1226 - sparse_categorical_accuracy: 0.7355\n",
      "Epoch 00002: val_sparse_categorical_accuracy improved from 0.70250 to 0.77125, saving model to ./checkpoints/mfcc\\chkp_best_cnn_V_a_alpha=0.3\n",
      "INFO:tensorflow:Assets written to: ./checkpoints/mfcc\\chkp_best_cnn_V_a_alpha=0.3\\assets\n",
      "200/200 [==============================] - 4s 22ms/step - loss: 1.1226 - sparse_categorical_accuracy: 0.7355 - val_loss: 0.9396 - val_sparse_categorical_accuracy: 0.7713\n",
      "Epoch 3/20\n",
      "196/200 [============================>.] - ETA: 0s - loss: 0.8008 - sparse_categorical_accuracy: 0.8125\n",
      "Epoch 00003: val_sparse_categorical_accuracy improved from 0.77125 to 0.84375, saving model to ./checkpoints/mfcc\\chkp_best_cnn_V_a_alpha=0.3\n",
      "INFO:tensorflow:Assets written to: ./checkpoints/mfcc\\chkp_best_cnn_V_a_alpha=0.3\\assets\n",
      "200/200 [==============================] - 5s 23ms/step - loss: 0.7992 - sparse_categorical_accuracy: 0.8131 - val_loss: 0.6894 - val_sparse_categorical_accuracy: 0.8438\n",
      "Epoch 4/20\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.6279 - sparse_categorical_accuracy: 0.8386\n",
      "Epoch 00004: val_sparse_categorical_accuracy improved from 0.84375 to 0.85875, saving model to ./checkpoints/mfcc\\chkp_best_cnn_V_a_alpha=0.3\n",
      "INFO:tensorflow:Assets written to: ./checkpoints/mfcc\\chkp_best_cnn_V_a_alpha=0.3\\assets\n",
      "200/200 [==============================] - 4s 20ms/step - loss: 0.6279 - sparse_categorical_accuracy: 0.8386 - val_loss: 0.5506 - val_sparse_categorical_accuracy: 0.8587\n",
      "Epoch 5/20\n",
      "196/200 [============================>.] - ETA: 0s - loss: 0.5221 - sparse_categorical_accuracy: 0.8694\n",
      "Epoch 00005: val_sparse_categorical_accuracy improved from 0.85875 to 0.87500, saving model to ./checkpoints/mfcc\\chkp_best_cnn_V_a_alpha=0.3\n",
      "INFO:tensorflow:Assets written to: ./checkpoints/mfcc\\chkp_best_cnn_V_a_alpha=0.3\\assets\n",
      "200/200 [==============================] - 4s 21ms/step - loss: 0.5212 - sparse_categorical_accuracy: 0.8697 - val_loss: 0.4817 - val_sparse_categorical_accuracy: 0.8750\n",
      "Epoch 6/20\n",
      "199/200 [============================>.] - ETA: 0s - loss: 0.4471 - sparse_categorical_accuracy: 0.8872\n",
      "Epoch 00006: val_sparse_categorical_accuracy improved from 0.87500 to 0.89250, saving model to ./checkpoints/mfcc\\chkp_best_cnn_V_a_alpha=0.3\n",
      "INFO:tensorflow:Assets written to: ./checkpoints/mfcc\\chkp_best_cnn_V_a_alpha=0.3\\assets\n",
      "200/200 [==============================] - 4s 21ms/step - loss: 0.4477 - sparse_categorical_accuracy: 0.8867 - val_loss: 0.4126 - val_sparse_categorical_accuracy: 0.8925\n",
      "Epoch 7/20\n",
      "196/200 [============================>.] - ETA: 0s - loss: 0.3955 - sparse_categorical_accuracy: 0.8992\n",
      "Epoch 00007: val_sparse_categorical_accuracy did not improve from 0.89250\n",
      "200/200 [==============================] - 2s 11ms/step - loss: 0.3945 - sparse_categorical_accuracy: 0.8992 - val_loss: 0.4476 - val_sparse_categorical_accuracy: 0.8725\n",
      "Epoch 8/20\n",
      "199/200 [============================>.] - ETA: 0s - loss: 0.3532 - sparse_categorical_accuracy: 0.9077\n",
      "Epoch 00008: val_sparse_categorical_accuracy did not improve from 0.89250\n",
      "200/200 [==============================] - 2s 11ms/step - loss: 0.3532 - sparse_categorical_accuracy: 0.9075 - val_loss: 0.4165 - val_sparse_categorical_accuracy: 0.8775\n",
      "Epoch 9/20\n",
      "198/200 [============================>.] - ETA: 0s - loss: 0.3233 - sparse_categorical_accuracy: 0.9108\n",
      "Epoch 00009: val_sparse_categorical_accuracy did not improve from 0.89250\n",
      "200/200 [==============================] - 2s 11ms/step - loss: 0.3228 - sparse_categorical_accuracy: 0.9111 - val_loss: 0.4059 - val_sparse_categorical_accuracy: 0.8850\n",
      "Epoch 10/20\n",
      "198/200 [============================>.] - ETA: 0s - loss: 0.2969 - sparse_categorical_accuracy: 0.9190\n",
      "Epoch 00010: val_sparse_categorical_accuracy did not improve from 0.89250\n",
      "200/200 [==============================] - 2s 11ms/step - loss: 0.2972 - sparse_categorical_accuracy: 0.9191 - val_loss: 0.3853 - val_sparse_categorical_accuracy: 0.8875\n",
      "Epoch 11/20\n",
      "197/200 [============================>.] - ETA: 0s - loss: 0.2735 - sparse_categorical_accuracy: 0.9264\n",
      "Epoch 00011: val_sparse_categorical_accuracy improved from 0.89250 to 0.90375, saving model to ./checkpoints/mfcc\\chkp_best_cnn_V_a_alpha=0.3\n",
      "INFO:tensorflow:Assets written to: ./checkpoints/mfcc\\chkp_best_cnn_V_a_alpha=0.3\\assets\n",
      "200/200 [==============================] - 4s 21ms/step - loss: 0.2720 - sparse_categorical_accuracy: 0.9266 - val_loss: 0.3375 - val_sparse_categorical_accuracy: 0.9038\n",
      "Epoch 12/20\n",
      "197/200 [============================>.] - ETA: 0s - loss: 0.2514 - sparse_categorical_accuracy: 0.9315\n",
      "Epoch 00012: val_sparse_categorical_accuracy did not improve from 0.90375\n",
      "200/200 [==============================] - 2s 11ms/step - loss: 0.2522 - sparse_categorical_accuracy: 0.9308 - val_loss: 0.3382 - val_sparse_categorical_accuracy: 0.8950\n",
      "Epoch 13/20\n",
      "199/200 [============================>.] - ETA: 0s - loss: 0.2344 - sparse_categorical_accuracy: 0.9358\n",
      "Epoch 00013: val_sparse_categorical_accuracy improved from 0.90375 to 0.91375, saving model to ./checkpoints/mfcc\\chkp_best_cnn_V_a_alpha=0.3\n",
      "INFO:tensorflow:Assets written to: ./checkpoints/mfcc\\chkp_best_cnn_V_a_alpha=0.3\\assets\n",
      "200/200 [==============================] - 4s 21ms/step - loss: 0.2345 - sparse_categorical_accuracy: 0.9358 - val_loss: 0.3048 - val_sparse_categorical_accuracy: 0.9137\n",
      "Epoch 14/20\n",
      "198/200 [============================>.] - ETA: 0s - loss: 0.2143 - sparse_categorical_accuracy: 0.9452\n",
      "Epoch 00014: val_sparse_categorical_accuracy improved from 0.91375 to 0.92000, saving model to ./checkpoints/mfcc\\chkp_best_cnn_V_a_alpha=0.3\n",
      "INFO:tensorflow:Assets written to: ./checkpoints/mfcc\\chkp_best_cnn_V_a_alpha=0.3\\assets\n",
      "200/200 [==============================] - 5s 23ms/step - loss: 0.2133 - sparse_categorical_accuracy: 0.9456 - val_loss: 0.3071 - val_sparse_categorical_accuracy: 0.9200\n",
      "Epoch 15/20\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.2022 - sparse_categorical_accuracy: 0.9450\n",
      "Epoch 00015: val_sparse_categorical_accuracy did not improve from 0.92000\n",
      "200/200 [==============================] - 2s 11ms/step - loss: 0.2022 - sparse_categorical_accuracy: 0.9450 - val_loss: 0.2997 - val_sparse_categorical_accuracy: 0.9125\n",
      "Epoch 16/20\n",
      "199/200 [============================>.] - ETA: 0s - loss: 0.1880 - sparse_categorical_accuracy: 0.9513\n",
      "Epoch 00016: val_sparse_categorical_accuracy did not improve from 0.92000\n",
      "200/200 [==============================] - 2s 11ms/step - loss: 0.1885 - sparse_categorical_accuracy: 0.9508 - val_loss: 0.3796 - val_sparse_categorical_accuracy: 0.8813\n",
      "Epoch 17/20\n",
      "196/200 [============================>.] - ETA: 0s - loss: 0.1762 - sparse_categorical_accuracy: 0.9528\n",
      "Epoch 00017: val_sparse_categorical_accuracy did not improve from 0.92000\n",
      "200/200 [==============================] - 2s 11ms/step - loss: 0.1763 - sparse_categorical_accuracy: 0.9527 - val_loss: 0.3238 - val_sparse_categorical_accuracy: 0.9062\n",
      "Epoch 18/20\n",
      "198/200 [============================>.] - ETA: 0s - loss: 0.1655 - sparse_categorical_accuracy: 0.9545\n",
      "Epoch 00018: val_sparse_categorical_accuracy did not improve from 0.92000\n",
      "200/200 [==============================] - 2s 12ms/step - loss: 0.1658 - sparse_categorical_accuracy: 0.9541 - val_loss: 0.3728 - val_sparse_categorical_accuracy: 0.9025\n",
      "Epoch 19/20\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.1528 - sparse_categorical_accuracy: 0.9598\n",
      "Epoch 00019: val_sparse_categorical_accuracy did not improve from 0.92000\n",
      "200/200 [==============================] - 2s 11ms/step - loss: 0.1528 - sparse_categorical_accuracy: 0.9598 - val_loss: 0.3074 - val_sparse_categorical_accuracy: 0.9112\n",
      "Epoch 20/20\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.1495 - sparse_categorical_accuracy: 0.9609\n",
      "Epoch 00020: val_sparse_categorical_accuracy did not improve from 0.92000\n",
      "200/200 [==============================] - 2s 11ms/step - loss: 0.1495 - sparse_categorical_accuracy: 0.9609 - val_loss: 0.3023 - val_sparse_categorical_accuracy: 0.9137\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_ds, epochs=20,   validation_data=val_ds,callbacks=[model_checkpoint_callback ])\n",
    "\n",
    "############################## Print Model Summary ####################\n",
    "print(model.summary())    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "90893b43-bb59-4d46-b1ca-dda5dfa404f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def plot_loss(history):\n",
    "    plt.plot(history.history['sparse_categorical_accuracy'], label='Accuracy')\n",
    "    plt.plot(history.history['val_sparse_categorical_accuracy'], label='val_Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(mymodel+\".png\")\n",
    "\n",
    "plot_loss(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c2a1e7-9a12-4464-b7b8-2d79a1d8b408",
   "metadata": {},
   "source": [
    "######################################################## Function To Evaluate the best model and convert to TF Lite ########################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b47c8f47-0ddb-4a4b-b7e7-305e29224ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def S_pruning_Model_evaluate_and_compress_to_TFlite( tflite_model_dir =  TFLITE , checkpoint_filepath = checkpoint_filepath ):\n",
    "    if not os.path.exists('./models'):\n",
    "        os.makedirs('./models')\n",
    "    best_model = tf.keras.models.load_model(filepath = checkpoint_filepath )\n",
    "    Loss , ACCURACY = best_model.evaluate(test_ds)\n",
    "    print(\"*\"*50,\"\\n\",f\" The accuracy achieved by the best model before convertion = {ACCURACY *100:0.2f}% \")\n",
    "    # Convert to TF lite without Quantization \n",
    "    converter = tf.lite.TFLiteConverter.from_saved_model(checkpoint_filepath)\n",
    "    tflite_model = converter.convert()  \n",
    "    Compressed = \"compressed_\"+tflite_model_dir \n",
    "    tflite_model_dir = './models/'+tflite_model_dir\n",
    "    # Write the model in binary formate and save it \n",
    "    with open(tflite_model_dir, 'wb') as fp:\n",
    "        fp.write(tflite_model)\n",
    "    Compressed = './models/'+Compressed\n",
    "    with open(Compressed, 'wb') as fp:\n",
    "        tflite_compressed = zlib.compress(tflite_model)\n",
    "        fp.write(tflite_compressed)\n",
    "    print(\"*\"*50,\"\\n\",f\"the model is saved successfuly to {tflite_model_dir}\")\n",
    "    return Compressed , tflite_model_dir "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ca895d30-0632-45d7-b398-f4b249344168",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getsize(file):\n",
    "    st = os.stat(file)\n",
    "    size = st.st_size\n",
    "    return size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98305d5c-d825-438e-9157-2b26d89c5491",
   "metadata": {},
   "source": [
    "######################################################## Function To Load  Evaluate the TF Lite  Model ########################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6c492b8c-cad1-456a-90aa-f74063e2d17f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_evaluation(path, dataset , Compressed):\n",
    "    interpreter = tf.lite.Interpreter(model_path = path) \n",
    "    interpreter.allocate_tensors()\n",
    "    input_details = interpreter.get_input_details()\n",
    "    output_details = interpreter.get_output_details()\n",
    "\n",
    "    dataset = test_ds.unbatch().batch(1)\n",
    "    \n",
    "    COMMANDS = ['stop', 'up', 'yes', 'right', 'left', 'no',  'down', 'go']\n",
    "    \n",
    "    outputs = []\n",
    "    labels = []\n",
    "    count = 0                                 # counter to compute the number of correct predictions \n",
    "    total = 0                                 # total number of samples / predictions ==> acc = count/total\n",
    "    \n",
    "    for inp , label in dataset:\n",
    "        my_input = np.array(inp, dtype = np.float32)\n",
    "        label = np.array(label, dtype = np.float32)\n",
    "    \n",
    "         \n",
    "        labels.append(label)\n",
    "\n",
    "        interpreter.set_tensor(input_details[0]['index'], my_input)\n",
    "        interpreter.invoke()\n",
    "        my_output = interpreter.get_tensor(output_details[0]['index'])\n",
    "        predict = np.argmax(my_output)                                 # the prediction crossponds to the index of with the highest probability   \n",
    "        outputs.append(predict)\n",
    "        total += 1   \n",
    "        if (predict == label):                                         # if probability == labesl increase the correct predictions counter \n",
    "            count += 1\n",
    "    # Compute the Accuracy         \n",
    "    accuracy = count/total \n",
    "    # Evaluate the size of Tflite model \n",
    "    size = getsize(path)\n",
    "    # Evaluate the size of Tflite model  after Comperession \n",
    "    size_compressed = getsize(Compressed)\n",
    "    print (\"*\"*50,\"\\n\",f\"The Size of TF lite model  Before compression is = {size /1000 } kb\" )\n",
    "    print (\"*\"*50,\"\\n\",f\"The Size of TF lite model  After compression is = {size_compressed /1000 } kb\" )\n",
    "    print (\"*\"*50,\"\\n\",f\"The accuracy of TF lite model is = {accuracy *100 :0.2f} \" )\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c0a9ba6-3980-4e5b-860e-c64a6457446e",
   "metadata": {},
   "source": [
    "######################################################## Generate Representitive data for Weight + activation Quantization ########################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "75d97a0e-6322-43c7-ab3e-944b1054ed92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for weight and activations quantization \n",
    "def representative_dataset_gen():\n",
    "    for x, _ in train_ds.take(1000):\n",
    "        yield [x]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8abfc31-dcde-4e5e-bd37-8fcf3ce6739e",
   "metadata": {},
   "source": [
    "######################################################## Apply quantization Function ########################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a13c1c41-3e6f-42ef-b454-e0f2dab2cb78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_Quantization(tflite_model_dir =  TFLITE ,  PQT = False , WAPQT = False ,  checkpoint_filepath = checkpoint_filepath ): \n",
    "\n",
    "    converter = tf.lite.TFLiteConverter.from_saved_model(checkpoint_filepath)\n",
    "    \n",
    "    # Apply weight only quantization \n",
    "    if PQT == True :\n",
    "        tflite_model_dir = f\"PQT_{tflite_model_dir}\"\n",
    "        converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "        tflite_model = converter.convert()\n",
    "    # Apply weight + Activation  quantization \n",
    "    if WAPQT == True :\n",
    "        converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "        converter.representative_dataset = representative_dataset_gen\n",
    "        tflite_model = converter.convert()\n",
    "        \n",
    "        tflite_model_dir = f\"WAPQT_{tflite_model_dir}\"\n",
    "    Compressed =  f\"compressed_{tflite_model_dir}\"\n",
    "    tflite_model_dir =   f\"./models/{tflite_model_dir}\"\n",
    "    # Write the model in binary formate and save it \n",
    "    with open(tflite_model_dir, 'wb') as fp:\n",
    "        fp.write(tflite_model)\n",
    "    Compressed = f\"./models/{Compressed}\"\n",
    "    with open(Compressed, 'wb') as fp:\n",
    "        tflite_compressed = zlib.compress(tflite_model)\n",
    "        fp.write(tflite_compressed)\n",
    "    print(f\"the model is saved successfuly to {tflite_model_dir}\")\n",
    "    return Compressed , tflite_model_dir "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a637db50-7fca-404a-885d-7ea383cfc01d",
   "metadata": {},
   "source": [
    "### Without Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "885f72c4-7c54-4342-8133-b278f685eed1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/25 [==============================] - 1s 45ms/step - loss: 0.3025 - sparse_categorical_accuracy: 0.9062\n",
      "************************************************** \n",
      "  The accuracy achieved by the best model before convertion = 90.62% \n",
      "************************************************** \n",
      " the model is saved successfuly to ./models/cnn_V_a_alpha=0.3.tflite\n"
     ]
    }
   ],
   "source": [
    "Compressed , tflite_model_dir = S_pruning_Model_evaluate_and_compress_to_TFlite( tflite_model_dir =  TFLITE , checkpoint_filepath = checkpoint_filepath )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "aca43301-6744-44cf-baaf-a70d1133af91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************************************************** \n",
      " The Size of TF lite model  Before compression is = 110.712 kb\n",
      "************************************************** \n",
      " The Size of TF lite model  After compression is = 101.117 kb\n",
      "************************************************** \n",
      " The accuracy of TF lite model is = 90.62 \n"
     ]
    }
   ],
   "source": [
    "load_and_evaluation(tflite_model_dir, test_ds , Compressed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7995ee4-ae8d-4bb7-82ba-1981df49aa49",
   "metadata": {},
   "source": [
    "### Weights only Quantization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6123d88d-1302-4a46-9735-530c1fd5211b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the model is saved successfuly to ./models/PQT_cnn_V_a_alpha=0.3.tflite\n"
     ]
    }
   ],
   "source": [
    "Compressed , Quantized   = apply_Quantization(PQT=True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "aa437620-ae23-4ed0-9b7b-347d33346097",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************************************************** \n",
      " The Size of TF lite model  Before compression is = 32.992 kb\n",
      "************************************************** \n",
      " The Size of TF lite model  After compression is = 26.67 kb\n",
      "************************************************** \n",
      " The accuracy of TF lite model is = 90.50 \n"
     ]
    }
   ],
   "source": [
    "load_and_evaluation(Quantized , test_ds , Compressed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9eb14e-69a4-4e11-a550-fde02dbe3a33",
   "metadata": {},
   "source": [
    "### Weights only Quantization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9509b77a-7a25-4e31-9b0b-02e5720e9dd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the model is saved successfuly to ./models/WAPQT_cnn_V_a_alpha=0.3.tflite\n"
     ]
    }
   ],
   "source": [
    "WA_Compressed , WA_Quantized   = apply_Quantization(WAPQT=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "307a9c2b-d850-4d12-a37d-dc5ffbcee821",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************************************************** \n",
      " The Size of TF lite model  Before compression is = 35.552 kb\n",
      "************************************************** \n",
      " The Size of TF lite model  After compression is = 28.582 kb\n",
      "************************************************** \n",
      " The accuracy of TF lite model is = 89.25 \n"
     ]
    }
   ],
   "source": [
    "load_and_evaluation(WA_Quantized , test_ds , WA_Compressed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afdffbb9-4c8a-434f-86bb-16a56d31b0c8",
   "metadata": {},
   "source": [
    "## Quantization aware Training :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "aad5d125-8e50-40ad-82a6-4768dad61acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_model_optimization as tfmot\n",
    "\n",
    "Q_aware_checkpoint_filepath = F'Q_aware_chkp_best_{mymodel}'\n",
    "    \n",
    "Q_aware_model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=Q_aware_checkpoint_filepath,           \n",
    "    monitor='val_sparse_categorical_accuracy',\n",
    "    verbose=1,\n",
    "    mode='max',\n",
    "    save_best_only=True,\n",
    "    save_freq='epoch')\n",
    "\n",
    "def Quantization_aware_traning(filepath = checkpoint_filepath , checkpoint_callback = Q_aware_model_checkpoint_callback ):\n",
    "\n",
    "    quantize_model = tfmot.quantization.keras.quantize_model\n",
    "    \n",
    "    # Retrieve the best pre_trained model float 32 \n",
    "    model = tf.keras.models.load_model(filepath = filepath )\n",
    "    \n",
    "    # Initiate a Quantization aware model from the Float 32 model to be trained \n",
    "    q_aware_model = quantize_model(model)\n",
    "    \n",
    "    # Model compile and define loss and metric \n",
    "    q_aware_model.compile(loss = loss, optimizer = optimizer, metrics = metrics)\n",
    "    \n",
    "    # Train the model for few epochs \n",
    "    q_aware_model_history = q_aware_model.fit(train_ds, epochs=10,   validation_data=val_ds,callbacks=[checkpoint_callback ])\n",
    "    \n",
    "    ############################## Print Model Summary ####################\n",
    "    print(model.summary())\n",
    "    \n",
    "    # Evaluate the best model \n",
    "    best_model = tf.keras.models.load_model(filepath = Q_aware_checkpoint_filepath )\n",
    "    Loss , ACCURACY = best_model.evaluate(test_ds)\n",
    "    print(\"*\"*50,\"\\n\",f\" The accuracy achieved by the best model before convertion = {ACCURACY *100:0.2f}% \")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b3680c-6534-42ba-a2c9-8adfb331b173",
   "metadata": {},
   "source": [
    "######################################################## Apply quantization Aware Training on the Pre Trained Model ########################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d03fa75b-4ae5-4f40-b7fd-84409811b71d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "198/200 [============================>.] - ETA: 0s - loss: 0.3836 - sparse_categorical_accuracy: 0.8847\n",
      "Epoch 00001: val_sparse_categorical_accuracy improved from -inf to 0.87250, saving model to Q_aware_chkp_best_cnn_V_a_alpha=0.3\n",
      "INFO:tensorflow:Assets written to: Q_aware_chkp_best_cnn_V_a_alpha=0.3\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Q_aware_chkp_best_cnn_V_a_alpha=0.3\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200/200 [==============================] - 7s 35ms/step - loss: 0.3812 - sparse_categorical_accuracy: 0.8851 - val_loss: 0.3968 - val_sparse_categorical_accuracy: 0.8725\n",
      "Epoch 2/10\n",
      "196/200 [============================>.] - ETA: 0s - loss: 0.2508 - sparse_categorical_accuracy: 0.9246\n",
      "Epoch 00002: val_sparse_categorical_accuracy improved from 0.87250 to 0.91375, saving model to Q_aware_chkp_best_cnn_V_a_alpha=0.3\n",
      "INFO:tensorflow:Assets written to: Q_aware_chkp_best_cnn_V_a_alpha=0.3\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Q_aware_chkp_best_cnn_V_a_alpha=0.3\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200/200 [==============================] - 6s 30ms/step - loss: 0.2492 - sparse_categorical_accuracy: 0.9250 - val_loss: 0.3140 - val_sparse_categorical_accuracy: 0.9137\n",
      "Epoch 3/10\n",
      "199/200 [============================>.] - ETA: 0s - loss: 0.2049 - sparse_categorical_accuracy: 0.9395\n",
      "Epoch 00003: val_sparse_categorical_accuracy improved from 0.91375 to 0.91875, saving model to Q_aware_chkp_best_cnn_V_a_alpha=0.3\n",
      "INFO:tensorflow:Assets written to: Q_aware_chkp_best_cnn_V_a_alpha=0.3\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Q_aware_chkp_best_cnn_V_a_alpha=0.3\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200/200 [==============================] - 6s 32ms/step - loss: 0.2050 - sparse_categorical_accuracy: 0.9395 - val_loss: 0.2681 - val_sparse_categorical_accuracy: 0.9187\n",
      "Epoch 4/10\n",
      "196/200 [============================>.] - ETA: 0s - loss: 0.1800 - sparse_categorical_accuracy: 0.9480\n",
      "Epoch 00004: val_sparse_categorical_accuracy did not improve from 0.91875\n",
      "200/200 [==============================] - 2s 12ms/step - loss: 0.1798 - sparse_categorical_accuracy: 0.9484 - val_loss: 0.2643 - val_sparse_categorical_accuracy: 0.9125\n",
      "Epoch 5/10\n",
      "199/200 [============================>.] - ETA: 0s - loss: 0.1694 - sparse_categorical_accuracy: 0.9524\n",
      "Epoch 00005: val_sparse_categorical_accuracy did not improve from 0.91875\n",
      "200/200 [==============================] - 2s 12ms/step - loss: 0.1695 - sparse_categorical_accuracy: 0.9523 - val_loss: 0.3209 - val_sparse_categorical_accuracy: 0.9062\n",
      "Epoch 6/10\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.1524 - sparse_categorical_accuracy: 0.9603\n",
      "Epoch 00006: val_sparse_categorical_accuracy did not improve from 0.91875\n",
      "200/200 [==============================] - 2s 12ms/step - loss: 0.1524 - sparse_categorical_accuracy: 0.9603 - val_loss: 0.3017 - val_sparse_categorical_accuracy: 0.9075\n",
      "Epoch 7/10\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.1401 - sparse_categorical_accuracy: 0.9619\n",
      "Epoch 00007: val_sparse_categorical_accuracy improved from 0.91875 to 0.92125, saving model to Q_aware_chkp_best_cnn_V_a_alpha=0.3\n",
      "INFO:tensorflow:Assets written to: Q_aware_chkp_best_cnn_V_a_alpha=0.3\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Q_aware_chkp_best_cnn_V_a_alpha=0.3\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200/200 [==============================] - 7s 34ms/step - loss: 0.1401 - sparse_categorical_accuracy: 0.9619 - val_loss: 0.2798 - val_sparse_categorical_accuracy: 0.9212\n",
      "Epoch 8/10\n",
      "196/200 [============================>.] - ETA: 0s - loss: 0.1275 - sparse_categorical_accuracy: 0.9668\n",
      "Epoch 00008: val_sparse_categorical_accuracy improved from 0.92125 to 0.92250, saving model to Q_aware_chkp_best_cnn_V_a_alpha=0.3\n",
      "INFO:tensorflow:Assets written to: Q_aware_chkp_best_cnn_V_a_alpha=0.3\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Q_aware_chkp_best_cnn_V_a_alpha=0.3\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200/200 [==============================] - 6s 32ms/step - loss: 0.1276 - sparse_categorical_accuracy: 0.9670 - val_loss: 0.2678 - val_sparse_categorical_accuracy: 0.9225\n",
      "Epoch 9/10\n",
      "196/200 [============================>.] - ETA: 0s - loss: 0.1174 - sparse_categorical_accuracy: 0.9707\n",
      "Epoch 00009: val_sparse_categorical_accuracy did not improve from 0.92250\n",
      "200/200 [==============================] - 2s 12ms/step - loss: 0.1182 - sparse_categorical_accuracy: 0.9705 - val_loss: 0.2934 - val_sparse_categorical_accuracy: 0.9137\n",
      "Epoch 10/10\n",
      "197/200 [============================>.] - ETA: 0s - loss: 0.1068 - sparse_categorical_accuracy: 0.9767\n",
      "Epoch 00010: val_sparse_categorical_accuracy improved from 0.92250 to 0.92625, saving model to Q_aware_chkp_best_cnn_V_a_alpha=0.3\n",
      "INFO:tensorflow:Assets written to: Q_aware_chkp_best_cnn_V_a_alpha=0.3\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Q_aware_chkp_best_cnn_V_a_alpha=0.3\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200/200 [==============================] - 6s 32ms/step - loss: 0.1080 - sparse_categorical_accuracy: 0.9759 - val_loss: 0.2650 - val_sparse_categorical_accuracy: 0.9262\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Conv2D-1 (Conv2D)            (None, 24, 8, 38)         342       \n",
      "_________________________________________________________________\n",
      "Btch_Norm-1 (BatchNormalizat (None, 24, 8, 38)         152       \n",
      "_________________________________________________________________\n",
      "re_lu (ReLU)                 (None, 24, 8, 38)         0         \n",
      "_________________________________________________________________\n",
      "Conv2D-2 (Conv2D)            (None, 22, 6, 38)         12996     \n",
      "_________________________________________________________________\n",
      "Btch_Norm-2 (BatchNormalizat (None, 22, 6, 38)         152       \n",
      "_________________________________________________________________\n",
      "re_lu_1 (ReLU)               (None, 22, 6, 38)         0         \n",
      "_________________________________________________________________\n",
      "Conv2D-3 (Conv2D)            (None, 20, 4, 38)         12996     \n",
      "_________________________________________________________________\n",
      "Btch_Norm-3 (BatchNormalizat (None, 20, 4, 38)         152       \n",
      "_________________________________________________________________\n",
      "re_lu_2 (ReLU)               (None, 20, 4, 38)         0         \n",
      "_________________________________________________________________\n",
      "GlobalAveragePooling-Layer ( (None, 38)                0         \n",
      "_________________________________________________________________\n",
      "Output-Layer (Dense)         (None, 8)                 312       \n",
      "=================================================================\n",
      "Total params: 27,102\n",
      "Trainable params: 26,874\n",
      "Non-trainable params: 228\n",
      "_________________________________________________________________\n",
      "None\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.2291 - sparse_categorical_accuracy: 0.9250\n",
      "************************************************** \n",
      "  The accuracy achieved by the best model before convertion = 92.50% \n"
     ]
    }
   ],
   "source": [
    "Quantization_aware_traning(filepath = checkpoint_filepath , checkpoint_callback = Q_aware_model_checkpoint_callback )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "270fe75f-f891-4da8-a03f-6dc0947ce781",
   "metadata": {},
   "source": [
    "### Quantization Aware model saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3825a937-dc4d-4782-8aa1-9a7cd2e2d430",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Q_Aware_T_Tflite_save(filepath = Q_aware_checkpoint_filepath):\n",
    "    QAT_tflite_model_dir = \"Q_AWARE_Training_\"+TFLITE\n",
    "    converter = tf.lite.TFLiteConverter.from_saved_model(filepath)\n",
    "    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "    tflite_model = converter.convert()\n",
    "    Compressed = \"compressed_\"+QAT_tflite_model_dir \n",
    "    QAT_tflite_model_dir = './models/'+QAT_tflite_model_dir\n",
    "    # Write the model in binary formate and save it \n",
    "    with open(QAT_tflite_model_dir, 'wb') as fp:\n",
    "        fp.write(tflite_model)\n",
    "    Compressed = './models/'+Compressed\n",
    "    with open(Compressed, 'wb') as fp:\n",
    "        tflite_compressed = zlib.compress(tflite_model)\n",
    "        fp.write(tflite_compressed)\n",
    "    print(\"*\"*50,\"\\n\",f\"the model is saved successfuly to {QAT_tflite_model_dir}\")\n",
    "    return QAT_tflite_model_dir , Compressed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5911f465-cea6-4bd4-9514-cd99ca311a8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************************************************** \n",
      " the model is saved successfuly to ./models/Q_AWARE_Training_cnn_V_a_alpha=0.3.tflite\n"
     ]
    }
   ],
   "source": [
    "QAT_tflite_model_dir , Q_Aware_T_Compressed = Q_Aware_T_Tflite_save(filepath = Q_aware_checkpoint_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0f68e574-f976-42de-9aa4-6f81048d652b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Type is <class 'numpy.float32'>\n",
      "************************************************** \n",
      " The Size of TF lite model  Before compression is = 35.44 kb\n",
      "************************************************** \n",
      " The Size of TF lite model  After compression is = 27.74 kb\n",
      "************************************************** \n",
      " The accuracy of TF lite model is = 92.38 \n"
     ]
    }
   ],
   "source": [
    "load_and_evaluation(QAT_tflite_model_dir, test_ds , Q_Aware_T_Compressed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f7907d-61b2-4b5a-822e-14ea91a71bfb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
