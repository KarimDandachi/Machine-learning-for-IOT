{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import argparse\r\n",
    "import numpy as np\r\n",
    "import os\r\n",
    "import pandas as pd\r\n",
    "import tensorflow as tf\r\n",
    "import tensorflow.lite as tflite\r\n",
    "from tensorflow import keras\r\n",
    "import zlib\r\n",
    "from platform import python_version\r\n",
    "import tensorflow_model_optimization as tfmot   \r\n",
    "import tempfile\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import tensorflow_model_optimization as tfmot\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "class read_audios:\r\n",
    "    def __init__(self):\r\n",
    "        pass\r\n",
    "    \r\n",
    "    def read(self):\r\n",
    "        train_file = open('kws_train_split.txt', \"r\") #opens the file in read mode\r\n",
    "        train_lines = train_file.read().splitlines() #puts the file into an array\r\n",
    "        train_file.close()\r\n",
    "        for i in range(len(train_lines)):\r\n",
    "            train_lines[i] = train_lines[i][2:]\r\n",
    "        train_tf = tf.convert_to_tensor(train_lines)\r\n",
    "        \r\n",
    "        test_file = open('kws_test_split.txt', \"r\") #opens the file in read mode\r\n",
    "        test_lines = test_file.read().splitlines() #puts the file into an array\r\n",
    "        test_file.close()\r\n",
    "        for i in range(len(test_lines)):\r\n",
    "            test_lines[i] = test_lines[i][2:]\r\n",
    "        test_tf = tf.convert_to_tensor(test_lines)\r\n",
    "        \r\n",
    "        val_file = open('kws_val_split.txt', \"r\") #opens the file in read mode\r\n",
    "        val_lines = val_file.read().splitlines() #puts the file into an array\r\n",
    "        val_file.close()\r\n",
    "        for i in range(len(val_lines)):\r\n",
    "            val_lines[i] = val_lines[i][2:]\r\n",
    "        val_tf = tf.convert_to_tensor(val_lines)\r\n",
    "        \r\n",
    "        return train_tf, val_tf, test_tf\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "class SignalGenerator:\r\n",
    "    def __init__(self, labels, sampling_rate, frame_length, frame_step,\r\n",
    "            num_mel_bins=None, lower_frequency=None, upper_frequency=None,\r\n",
    "            num_coefficients=None, mfcc=False):\r\n",
    "        self.labels = labels\r\n",
    "        self.sampling_rate = sampling_rate                                             # 16000  \r\n",
    "        self.frame_length = frame_length                                               # 640 \r\n",
    "        self.frame_step = frame_step                                                   # 320 \r\n",
    "        self.num_mel_bins = num_mel_bins                                               # 40 \r\n",
    "        self.lower_frequency = lower_frequency                                         # 20 \r\n",
    "        self.upper_frequency = upper_frequency                                         # 4000\r\n",
    "        self.num_coefficients = num_coefficients                                       # 10 \r\n",
    "        num_spectrogram_bins = (frame_length) // 2 + 1                                  # ( frame size // 2 ) + 1 \r\n",
    "\r\n",
    "        '''\r\n",
    "        STFT_OPTIONS = {'frame_length': 256, 'frame_step': 128, 'mfcc': False}\r\n",
    "        MFCC_OPTIONS = {'frame_length': 640, 'frame_step': 320, 'mfcc': True,\r\n",
    "        'lower_frequency': 20, 'upper_frequency': 4000, 'num_mel_bins': 40,\r\n",
    "        'num_coefficients': 10}\r\n",
    "        '''\r\n",
    "\r\n",
    "        if mfcc is True:                                                                # Remember we need to compute this matrix once so it will be a class argument \r\n",
    "            self.linear_to_mel_weight_matrix = tf.signal.linear_to_mel_weight_matrix(\r\n",
    "                    self.num_mel_bins, num_spectrogram_bins, self.sampling_rate,\r\n",
    "                    self.lower_frequency, self.upper_frequency)\r\n",
    "            self.preprocess = self.preprocess_with_mfcc\r\n",
    "        else:\r\n",
    "            self.preprocess = self.preprocess_with_stft\r\n",
    "\r\n",
    "    def read(self, file_path):\r\n",
    "        parts = tf.strings.split(file_path, os.path.sep)\r\n",
    "        label = parts[-2]                                  # -1 is audio.wav so \r\n",
    "        label_id = tf.argmax(label == self.labels)\r\n",
    "        audio_binary = tf.io.read_file(file_path)\r\n",
    "        audio, _ = tf.audio.decode_wav(audio_binary)\r\n",
    "        audio = tf.squeeze(audio, axis=1)\r\n",
    "\r\n",
    "        return audio, label_id\r\n",
    "\r\n",
    "    def pad(self, audio):\r\n",
    "        # Padding for files with less than 16000 samples\r\n",
    "        zero_padding = tf.zeros([self.sampling_rate] - tf.shape(audio), dtype=tf.float32)     # if the shape of the audio is already = 16000 (sampling rate) we will add nothing \r\n",
    "\r\n",
    "        # Concatenate audio with padding so that all audio clips will be of the  same length\r\n",
    "        audio = tf.concat([audio, zero_padding], 0)\r\n",
    "        # Unify the shape to the sampling frequency (16000 , )\r\n",
    "        audio.set_shape([self.sampling_rate])\r\n",
    "\r\n",
    "        return audio\r\n",
    "\r\n",
    "    def get_spectrogram(self, audio):\r\n",
    "        stft = tf.signal.stft(audio, frame_length=self.frame_length,\r\n",
    "                frame_step=self.frame_step, fft_length=self.frame_length)\r\n",
    "        spectrogram = tf.abs(stft)\r\n",
    "\r\n",
    "        return spectrogram\r\n",
    "\r\n",
    "    def get_mfccs(self, spectrogram):\r\n",
    "        mel_spectrogram = tf.tensordot(spectrogram,\r\n",
    "                self.linear_to_mel_weight_matrix, 1)\r\n",
    "        log_mel_spectrogram = tf.math.log(mel_spectrogram + 1.e-6)\r\n",
    "        mfccs = tf.signal.mfccs_from_log_mel_spectrograms(log_mel_spectrogram)\r\n",
    "        mfccs = mfccs[..., :self.num_coefficients]\r\n",
    "\r\n",
    "        return mfccs\r\n",
    "\r\n",
    "    def preprocess_with_stft(self, file_path):\r\n",
    "        audio, label = self.read(file_path)\r\n",
    "        audio = self.pad(audio)\r\n",
    "        spectrogram = self.get_spectrogram(audio)\r\n",
    "        spectrogram = tf.expand_dims(spectrogram, -1)                         # expand_dims will not add or reduce elements in a tensor, it just changes the shape by adding 1 to dimensions for the batchs. \r\n",
    "    \r\n",
    "        spectrogram = tf.image.resize(spectrogram, [32, 32])\r\n",
    "\r\n",
    "        return spectrogram, label\r\n",
    "\r\n",
    "    def preprocess_with_mfcc(self, file_path):\r\n",
    "        audio, label = self.read(file_path)\r\n",
    "        audio = self.pad(audio)\r\n",
    "        spectrogram = self.get_spectrogram(audio)\r\n",
    "        mfccs = self.get_mfccs(spectrogram)\r\n",
    "        mfccs = tf.expand_dims(mfccs, -1)\r\n",
    "\r\n",
    "        return mfccs, label\r\n",
    "\r\n",
    "    def make_dataset(self, files, train):\r\n",
    "        ds = tf.data.Dataset.from_tensor_slices(files)\r\n",
    "        ds = ds.map(self.preprocess, num_parallel_calls = tf.data.experimental.AUTOTUNE) # better than 4 tf.data.experimental.AUTOTUNE will use the maximum num_parallel_calls \r\n",
    "        ds = ds.batch(32)\r\n",
    "        ds = ds.cache()\r\n",
    "        ds = ds.prefetch(tf.data.experimental.AUTOTUNE)\r\n",
    "        if train is True:\r\n",
    "            ds = ds.shuffle(100, reshuffle_each_iteration=True)\r\n",
    "\r\n",
    "        return ds\r\n",
    "\r\n",
    "\r\n",
    "class make_models():\r\n",
    "    def __init__(self):\r\n",
    "        pass\r\n",
    "\r\n",
    "    def models(self,alpha, strides, units, mfcc, mymodel, magnitude = False, train_ds = None):\r\n",
    "        mlp = tf.keras.Sequential([\r\n",
    "            tf.keras.layers.Flatten(),\r\n",
    "            tf.keras.layers.Dense(units = int(256 *alpha), activation='relu' , name =  \"Dense-1\" ),\r\n",
    "            tf.keras.layers.Dense(units = int(256 *alpha), activation='relu', name =  \"Dense-2\"),\r\n",
    "            tf.keras.layers.Dense(units = int(256 *alpha), activation='relu', name =   \"Dense-3\" ),\r\n",
    "            tf.keras.layers.Dense(units = units , name =  \"Output-Layer\")                                   # change to 9 if silence included \r\n",
    "        ])\r\n",
    "\r\n",
    "        cnn = tf.keras.Sequential([\r\n",
    "            tf.keras.layers.Conv2D(filters=int(128 *alpha), kernel_size=[3,3], strides=strides, use_bias=False , name = \"Conv2D-1\"),\r\n",
    "            tf.keras.layers.BatchNormalization(momentum=0.1 , name = \"Btch_Norm-1\"),\r\n",
    "            tf.keras.layers.ReLU(),\r\n",
    "            tf.keras.layers.Conv2D(filters=int(128 *alpha), kernel_size=[3,3], strides=[1,1], use_bias=False , name = \"Conv2D-2\"),\r\n",
    "            tf.keras.layers.BatchNormalization(momentum=0.1 , name = \"Btch_Norm-2\"),\r\n",
    "            tf.keras.layers.ReLU(),\r\n",
    "            tf.keras.layers.Conv2D(filters=int(128 *alpha), kernel_size=[3,3], strides=[1,1], use_bias=False , name = \"Conv2D-3\"),\r\n",
    "            tf.keras.layers.BatchNormalization(momentum=0.1 , name = \"Btch_Norm-3\"),\r\n",
    "            tf.keras.layers.ReLU(),\r\n",
    "            tf.keras.layers.GlobalAveragePooling2D( name =  \"GlobalAveragePooling-Layer\"),\r\n",
    "            tf.keras.layers.Dense(units = units, name =  \"Output-Layer\")\r\n",
    "        ])\r\n",
    "\r\n",
    "        ds_cnn = tf.keras.Sequential([\r\n",
    "            tf.keras.layers.Conv2D(filters=int(256 *alpha), kernel_size=[3,3], strides=strides, use_bias=False, name = \"Conv2D-1\"),\r\n",
    "            tf.keras.layers.BatchNormalization(momentum=0.1),\r\n",
    "            tf.keras.layers.ReLU(),\r\n",
    "            tf.keras.layers.DepthwiseConv2D(kernel_size=[3, 3], strides=[1, 1], use_bias=False, name = \"DepthwiseConv2D-1\"),\r\n",
    "            tf.keras.layers.Conv2D(filters=int(256 *alpha), kernel_size=[1,1], strides=[1,1], use_bias=False, name = \"Conv2D-2\"),\r\n",
    "            tf.keras.layers.BatchNormalization(momentum=0.1),\r\n",
    "            tf.keras.layers.ReLU(),\r\n",
    "            tf.keras.layers.DepthwiseConv2D(kernel_size=[3, 3], strides=[1, 1], use_bias=False, name = \"DepthwiseConv2D-2\"),\r\n",
    "            tf.keras.layers.Conv2D(filters=int(256 *alpha), kernel_size=[1,1], strides=[1,1], use_bias=False, name = \"Conv2D-3\"),\r\n",
    "            tf.keras.layers.BatchNormalization(momentum=0.1),\r\n",
    "            tf.keras.layers.ReLU(),\r\n",
    "            tf.keras.layers.GlobalAveragePooling2D( name =  \"GlobalAveragePooling-Layer\"),\r\n",
    "            tf.keras.layers.Dense(units = units, name =  \"Output-Layer\")\r\n",
    "        ])\r\n",
    "\r\n",
    "\r\n",
    "        MODELS = {'mlp'+ mymodel : mlp, 'cnn'+ mymodel: cnn, 'ds_cnn'+ mymodel: ds_cnn}\r\n",
    "        model = MODELS['ds_cnn' + mymodel] \r\n",
    "        loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)\r\n",
    "        optimizer = tf.optimizers.Adam()\r\n",
    "        metrics = [tf.keras.metrics.SparseCategoricalAccuracy()]\r\n",
    "        model.compile(loss = loss, optimizer = optimizer, metrics = metrics)\r\n",
    "\r\n",
    "        if mfcc is False:\r\n",
    "            checkpoint_filepath = f'./checkpoints/stft/chkp_best_{mymodel}'\r\n",
    "\r\n",
    "        else:\r\n",
    "            checkpoint_filepath = f'./checkpoints/mfcc/chkp_best_{mymodel}'\r\n",
    "        if not magnitude:\r\n",
    "            model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\r\n",
    "                filepath=checkpoint_filepath,           \r\n",
    "                monitor='val_sparse_categorical_accuracy',\r\n",
    "                verbose=1,\r\n",
    "                mode='max',\r\n",
    "                save_best_only=True,\r\n",
    "                save_freq='epoch')\r\n",
    "        else:\r\n",
    "            pruning_params = {'pruning_schedule':\r\n",
    "            tfmot.sparsity.keras.PolynomialDecay(\r\n",
    "            initial_sparsity=0.30,\r\n",
    "            final_sparsity=0.8,\r\n",
    "            begin_step=len(train_ds)*5,\r\n",
    "            end_step=len(train_ds)*15)\r\n",
    "            }\r\n",
    "            prune_low_magnitude = tfmot.sparsity.keras.prune_low_magnitude\r\n",
    "            model = prune_low_magnitude(model, **pruning_params)\r\n",
    "            model_checkpoint_callback = tfmot.sparsity.keras.UpdatePruningStep()\r\n",
    "        return model, model_checkpoint_callback, checkpoint_filepath\r\n",
    "\r\n",
    "    def plot_loss(slef, history, mymodel,i):\r\n",
    "        plt.plot(history.history['sparse_categorical_accuracy'], label='accuracy for version:{0}'.format(i))\r\n",
    "        #plt.plot(history.history['val_sparse_categorical_accuracy'], label='val_Accuracy')\r\n",
    "        plt.xlabel('Epoch')\r\n",
    "        plt.ylabel('Accuracy')\r\n",
    "        plt.legend()\r\n",
    "        plt.grid(True)\r\n",
    "        #plt.show()\r\n",
    "        plt.savefig(mymodel+\".png\")\r\n",
    "        if i == 3:\r\n",
    "           plt.show() \r\n",
    "        \r\n",
    "\r\n",
    "\r\n",
    "    \r\n",
    "\r\n",
    "class model_analysis():\r\n",
    "    def __init__(self, test_ds, checkpoint_filepath, train_ds):\r\n",
    "        self.test_ds = test_ds\r\n",
    "        self.checkpoint_filepath = checkpoint_filepath\r\n",
    "        self.train_ds = train_ds\r\n",
    "\r\n",
    "    def S_pruning_Model_evaluate_and_compress_to_TFlite(self, tflite_model_dir):\r\n",
    "        if not os.path.exists('./models'):\r\n",
    "            os.makedirs('./models')\r\n",
    "        if not os.path.exists('./compressed_models/'):\r\n",
    "            os.makedirs('./compressed_models/')\r\n",
    "        best_model = tf.keras.models.load_model(filepath = self.checkpoint_filepath )\r\n",
    "        Loss , ACCURACY = best_model.evaluate(self.test_ds)\r\n",
    "        # Convert to TF lite without Quantization \r\n",
    "        converter = tf.lite.TFLiteConverter.from_saved_model(self.checkpoint_filepath)\r\n",
    "        tflite_model = converter.convert()  \r\n",
    "        Compressed = \"compressed_\"+tflite_model_dir \r\n",
    "        tflite_model_dir = './models/'+tflite_model_dir\r\n",
    "        # Write the model in binary formate and save it \r\n",
    "        with open(tflite_model_dir, 'wb') as fp:\r\n",
    "            fp.write(tflite_model)\r\n",
    "        Compressed = './compressed_models/'+Compressed\r\n",
    "        with open(Compressed, 'wb') as fp:\r\n",
    "            tflite_compressed = zlib.compress(tflite_model)\r\n",
    "            fp.write(tflite_compressed)\r\n",
    "        return Compressed , tflite_model_dir \r\n",
    "\r\n",
    "    def getsize(self, file):\r\n",
    "        st = os.stat(file)\r\n",
    "        size = st.st_size\r\n",
    "        return size\r\n",
    "\r\n",
    "    def load_and_evaluation(self, path , Compressed):\r\n",
    "        interpreter = tf.lite.Interpreter(model_path = path) \r\n",
    "        interpreter.allocate_tensors()\r\n",
    "        input_details = interpreter.get_input_details()\r\n",
    "        output_details = interpreter.get_output_details()\r\n",
    "\r\n",
    "        dataset = self.test_ds.unbatch().batch(1)\r\n",
    "        \r\n",
    "        COMMANDS = ['stop', 'up', 'yes', 'right', 'left', 'no',  'down', 'go']\r\n",
    "        \r\n",
    "        outputs = []\r\n",
    "        labels = []\r\n",
    "        count = 0                                 # counter to compute the number of correct predictions \r\n",
    "        total = 0                                 # total number of samples / predictions ==> acc = count/total\r\n",
    "        \r\n",
    "        for inp , label in dataset:\r\n",
    "            my_input = np.array(inp, dtype = np.float32)\r\n",
    "            label = np.array(label, dtype = np.float32)\r\n",
    "        \r\n",
    "            \r\n",
    "            labels.append(label)\r\n",
    "\r\n",
    "            interpreter.set_tensor(input_details[0]['index'], my_input)\r\n",
    "            interpreter.invoke()\r\n",
    "            my_output = interpreter.get_tensor(output_details[0]['index'])\r\n",
    "            predict = np.argmax(my_output)                                 # the prediction crossponds to the index of with the highest probability   \r\n",
    "            outputs.append(predict)\r\n",
    "            total += 1   \r\n",
    "            if (predict == label):                                         # if probability == labesl increase the correct predictions counter \r\n",
    "                count += 1\r\n",
    "        # Compute the Accuracy         \r\n",
    "        accuracy = count/total*100\r\n",
    "        # Evaluate the size of Tflite model \r\n",
    "        size = self.getsize(path)/1000\r\n",
    "        # Evaluate the size of Tflite model  after Comperession \r\n",
    "        size_compressed = self.getsize(Compressed)/1000\r\n",
    "        return accuracy, size_compressed \r\n",
    "\r\n",
    "        # Function for weight and activations quantization \r\n",
    "    def representative_dataset_gen(self):\r\n",
    "        for x, _ in self.train_ds.take(1000):\r\n",
    "            yield [x]\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "    def apply_Quantization(self, tflite_model_dir, PQT = False, WAPQT = False): \r\n",
    "\r\n",
    "        converter = tf.lite.TFLiteConverter.from_saved_model(self.checkpoint_filepath)\r\n",
    "        \r\n",
    "        # Apply weight only quantization \r\n",
    "        if PQT == True :\r\n",
    "            tflite_model_dir = f\"PQT_{tflite_model_dir}\"\r\n",
    "            converter.optimizations = [tf.lite.Optimize.DEFAULT]\r\n",
    "            tflite_model = converter.convert()\r\n",
    "        # Apply weight + Activation  quantization \r\n",
    "        if WAPQT == True :\r\n",
    "            converter.optimizations = [tf.lite.Optimize.DEFAULT]\r\n",
    "            converter.representative_dataset = self.representative_dataset_gen\r\n",
    "            tflite_model = converter.convert()\r\n",
    "        \r\n",
    "            tflite_model_dir = f\"WAPQT_{tflite_model_dir}\"\r\n",
    "        Compressed =  f\"compressed_{tflite_model_dir}\"\r\n",
    "        tflite_model_dir =   f\"./models/{tflite_model_dir}\"\r\n",
    "        # Write the model in binary formate and save it \r\n",
    "        with open(tflite_model_dir, 'wb') as fp:\r\n",
    "            fp.write(tflite_model)\r\n",
    "        Compressed = f\"./models/{Compressed}\"\r\n",
    "        with open(Compressed, 'wb') as fp:\r\n",
    "            tflite_compressed = zlib.compress(tflite_model)\r\n",
    "            fp.write(tflite_compressed)\r\n",
    "        print(f\"the model is saved successfuly to {tflite_model_dir}\")\r\n",
    "        return Compressed , tflite_model_dir \r\n",
    "\r\n",
    "\r\n",
    "class latency():\r\n",
    "    def __init__(self):\r\n",
    "        pass\r\n",
    "\r\n",
    "    def calculate(self, model, rate = 16000, mfcc = False, resize = 32, length = 640, stride = 320, num_mel_bins = 40, lower_frequency = 20, upper_frequency = 4000, num_coefficients = 10):\r\n",
    "        import tensorflow as tf\r\n",
    "        import time\r\n",
    "        from scipy import signal\r\n",
    "        import numpy as np\r\n",
    "        from subprocess import call\r\n",
    "\r\n",
    "\r\n",
    "        num_frames = (rate - length) // stride + 1\r\n",
    "        num_spectrogram_bins = length // 2 + 1\r\n",
    "\r\n",
    "        linear_to_mel_weight_matrix = tf.signal.linear_to_mel_weight_matrix(\r\n",
    "                num_mel_bins, num_spectrogram_bins, rate, lower_frequency,\r\n",
    "                upper_frequency)\r\n",
    "\r\n",
    "        if model is not None:\r\n",
    "            interpreter = tf.lite.Interpreter(model_path = model)\r\n",
    "            interpreter.allocate_tensors()\r\n",
    "\r\n",
    "            input_details = interpreter.get_input_details()\r\n",
    "            output_details = interpreter.get_output_details()\r\n",
    "\r\n",
    "\r\n",
    "        inf_latency = []\r\n",
    "        tot_latency = []\r\n",
    "        for i in range(100):\r\n",
    "            sample = np.array(np.random.random_sample(48000), dtype=np.float32)\r\n",
    "\r\n",
    "            start = time.time()\r\n",
    "\r\n",
    "            # Resampling\r\n",
    "            sample = signal.resample_poly(sample, 1, 48000 // rate)\r\n",
    "\r\n",
    "            sample = tf.convert_to_tensor(sample, dtype=tf.float32)\r\n",
    "\r\n",
    "            # STFT\r\n",
    "            stft = tf.signal.stft(sample, length, stride,\r\n",
    "                    fft_length=length)\r\n",
    "            spectrogram = tf.abs(stft)\r\n",
    "\r\n",
    "            if mfcc is False and resize > 0:\r\n",
    "                # Resize (optional)\r\n",
    "                spectrogram = tf.reshape(spectrogram, [1, num_frames, num_spectrogram_bins, 1])\r\n",
    "                spectrogram = tf.image.resize(spectrogram, [resize, resize])\r\n",
    "                input_tensor = spectrogram\r\n",
    "            else:\r\n",
    "                # MFCC (optional)\r\n",
    "                mel_spectrogram = tf.tensordot(spectrogram, linear_to_mel_weight_matrix, 1)\r\n",
    "                log_mel_spectrogram = tf.math.log(mel_spectrogram + 1.e-6)\r\n",
    "                mfccs = tf.signal.mfccs_from_log_mel_spectrograms(log_mel_spectrogram)\r\n",
    "                mfccs = mfccs[..., :num_coefficients]\r\n",
    "                mfccs = tf.reshape(mfccs, [1, num_frames, num_coefficients, 1])\r\n",
    "                input_tensor = mfccs\r\n",
    "\r\n",
    "            if model is not None:\r\n",
    "                interpreter.set_tensor(input_details[0]['index'], input_tensor)\r\n",
    "                start_inf = time.time()\r\n",
    "                interpreter.invoke()\r\n",
    "                output_data = interpreter.get_tensor(output_details[0]['index'])\r\n",
    "\r\n",
    "            end = time.time()\r\n",
    "            tot_latency.append(end - start)\r\n",
    "\r\n",
    "            if model is None:\r\n",
    "                start_inf = end\r\n",
    "\r\n",
    "            inf_latency.append(end - start_inf)\r\n",
    "            time.sleep(0.1)\r\n",
    "\r\n",
    "        inf = np.mean(inf_latency)*1000.\r\n",
    "        tot = np.mean(tot_latency)*1000.\r\n",
    "        return inf, tot\r\n",
    "\r\n",
    "####################\r\n",
    "import argparse\r\n",
    "import numpy as np\r\n",
    "import os\r\n",
    "import pandas as pd\r\n",
    "import tensorflow as tf\r\n",
    "import tensorflow.lite as tflite\r\n",
    "from tensorflow import keras\r\n",
    "import zlib\r\n",
    "from platform import python_version\r\n",
    "import tensorflow_model_optimization as tfmot   \r\n",
    "import tempfile\r\n",
    "import time\r\n",
    "\r\n",
    "epochs = 1\r\n",
    "mymodel = \"model_\"\r\n",
    "TFLITE =  f'{mymodel}.tflite'     # path for saving the best model after converted to TF.lite model \r\n",
    "units = 8                         # The number of output class [8:without silence , 9 : with silence]\r\n",
    "################## Fix the Random seed to reproduce the same results \r\n",
    "seed = 42\r\n",
    "tf.random.set_seed(seed)\r\n",
    "np.random.seed(seed)\r\n",
    "zip_path = tf.keras.utils.get_file(\r\n",
    "     origin=\"http://storage.googleapis.com/download.tensorflow.org/data/mini_speech_commands.zip\",\r\n",
    "     fname='mini_speech_commands.zip',\r\n",
    "     extract=True,\r\n",
    "     cache_dir='.', cache_subdir='data')\r\n",
    "data_dir = os.path.join('.', 'data', 'mini_speech_commands')\r\n",
    "reading_class = read_audios()\r\n",
    "train_files, val_files, test_files = reading_class.read()\r\n",
    "alpha = [0.59]\r\n",
    "mfcc = [True]\r\n",
    "m = ['ds_cnn']\r\n",
    "number_of_bins = [16]\r\n",
    "lower_freq = [20]\r\n",
    "upper_freq = [4000]\r\n",
    "sample_rate = [16000]\r\n",
    "frame_length = [1000]\r\n",
    "frame_step = [500]\r\n",
    "\r\n",
    "# alpha = [0.3]\r\n",
    "# mfcc = [True]\r\n",
    "# m = ['cnn']\r\n",
    "# number_of_bins = [16]\r\n",
    "# lower_freq = [20]\r\n",
    "# upper_freq = [4000]\r\n",
    "# sample_rate = [16000]\r\n",
    "# frame_length = [1000]\r\n",
    "# frame_step = [100]\r\n",
    "df = pd.DataFrame(columns=['alpha', 'mfcc', 'model', 'number_of_bins', 'lower_freq', 'upper_freq','sample_rate','frame_step', 'frame_length','n_acuracy','n_size','n_latency','q_acuracy','q_size','q_latency'])\r\n",
    "for p_alpha in range(len(alpha)):\r\n",
    "    for p_mfcc in range(len(mfcc)):\r\n",
    "        for p_m in range(len(m)):\r\n",
    "            for p_bins in range(len(number_of_bins)):\r\n",
    "                for p_lower_f in range(len(lower_freq)):\r\n",
    "                    for p_upper_f in range(len(upper_freq)):\r\n",
    "                        for p_rate in range(len(sample_rate)):\r\n",
    "                            for p_f_step in range(len(frame_step)):\r\n",
    "                                for p_f_length in range(len(frame_length)):\r\n",
    "                                    if((lower_freq[p_lower_f] < upper_freq[p_upper_f]) and (upper_freq[p_lower_f] <= sample_rate[p_upper_f]/2) and (frame_step[p_f_step] < frame_length[p_f_length])):\r\n",
    "                                        data = [[alpha[p_alpha],mfcc[p_mfcc],m[p_m],number_of_bins[p_bins],lower_freq[p_lower_f],upper_freq[p_upper_f],sample_rate[p_rate],frame_step[p_f_step],frame_length[p_f_length],0,0,0,0,0,0]]\r\n",
    "                                        d = pd.DataFrame(data, columns=['alpha', 'mfcc', 'model', 'number_of_bins', 'lower_freq', 'upper_freq','sample_rate','frame_step', 'frame_length','n_acuracy','n_size','n_latency','q_acuracy','q_size','q_latency'])\r\n",
    "                                        df = df.append(d, ignore_index = True)\r\n",
    "\r\n",
    "LABELS = np.array(['stop', 'up', 'yes', 'right', 'left', 'no',  'down', 'go'] , dtype = str) \r\n",
    "start = 0\r\n",
    "end = 0\r\n",
    "for i in range(df.shape[0]):\r\n",
    "    print(\"configuration  \" + str(i) + \" from  \" + str(df.shape[0])+ \"!!!!!!!!!!!!\")\r\n",
    "    print(\"time estimate is:    \" + str((end - start)/60 *(df.shape[0] - i)) + \" minutes left\")\r\n",
    "    start = time.time()\r\n",
    "    \r\n",
    "    mymodel = \"V_\" + str(i)\r\n",
    "    TFLITE =  f'{mymodel}.tflite' \r\n",
    "\r\n",
    "    STFT_OPTIONS = {'frame_length': df['frame_length'][i], 'frame_step': df['frame_step'][i], 'mfcc': False}\r\n",
    "    MFCC_OPTIONS = { 'sampling_rate': df['sample_rate'][i],'frame_length': df['frame_length'][i], 'frame_step': df['frame_step'][i], 'mfcc': True,\r\n",
    "            'lower_frequency': df['lower_freq'][i], 'upper_frequency': df['upper_freq'][i], 'num_mel_bins': df['number_of_bins'][i],\r\n",
    "            'num_coefficients': 10}\r\n",
    "    if df.loc[i]['mfcc'] is True:\r\n",
    "        options = MFCC_OPTIONS\r\n",
    "        strides = [2, 1]\r\n",
    "    else:\r\n",
    "        options = STFT_OPTIONS\r\n",
    "        strides = [2, 2]\r\n",
    "\r\n",
    "    generator = SignalGenerator(LABELS, **options)\r\n",
    "    train_ds = generator.make_dataset(train_files, True)\r\n",
    "    val_ds = generator.make_dataset(val_files, False)\r\n",
    "    test_ds = generator.make_dataset(test_files, False)\r\n",
    "\r\n",
    "    model_maker = make_models()\r\n",
    "    ############ Applying Structured-Based Pruning\r\n",
    "    model, model_checkpoint_callback, checkpoint_filepath = model_maker.models(df['alpha'][i], strides, units, df['mfcc'][i], mymodel,False,train_ds)\r\n",
    "    ############ Applying Magnitude-Based Pruning\r\n",
    "    #model, model_checkpoint_callback, checkpoint_filepath = model_maker.models(1, strides, units, model_version, mfcc, mymodel)\r\n",
    "    history = model.fit(train_ds, epochs=epochs,   validation_data=val_ds,callbacks=[model_checkpoint_callback ], verbose=0)\r\n",
    "    #model_maker.plot_loss(history, mymodel)\r\n",
    "    analysis = model_analysis(test_ds, checkpoint_filepath, train_ds)\r\n",
    "    Compressed , tflite_model_dir = analysis.S_pruning_Model_evaluate_and_compress_to_TFlite( tflite_model_dir = TFLITE)\r\n",
    "    acc, size = analysis.load_and_evaluation(tflite_model_dir, Compressed)\r\n",
    "    laten = latency()\r\n",
    "    inf, tot = laten.calculate(model = tflite_model_dir, mfcc = df['mfcc'][i] ,rate = df['sample_rate'][i], length = df['frame_length'][i], stride = df['frame_step'][i],lower_frequency = df['lower_freq'][i], upper_frequency = df['upper_freq'][i], num_mel_bins = df['number_of_bins'][i])\r\n",
    "    df['n_acuracy'][i] = acc\r\n",
    "    df['n_size'][i] = size\r\n",
    "    df['n_latency'][i] = tot\r\n",
    "    print(\"normal done  $$$$$$$$$$$$$$$$$$$$$$$\")\r\n",
    "    #quantized:\r\n",
    "    Compressed , Quantized   = analysis.apply_Quantization(TFLITE, PQT=True , WAPQT = False)\r\n",
    "    acc, size = analysis.load_and_evaluation(Quantized , Compressed)\r\n",
    "    inf, tot = laten.calculate(model = Quantized, mfcc = df['mfcc'][i] ,rate = df['sample_rate'][i], length = df['frame_length'][i], stride = df['frame_step'][i],lower_frequency = df['lower_freq'][i], upper_frequency = df['upper_freq'][i], num_mel_bins = df['number_of_bins'][i])\r\n",
    "    df['q_acuracy'][i] = acc\r\n",
    "    df['q_size'][i] = size\r\n",
    "    df['q_latency'][i] = tot\r\n",
    "    print(\"quantized done  $$$$$$$$$$$$$$$$$$$$$$$\")\r\n",
    "    end = time.time()\r\n",
    "    \r\n",
    "\r\n",
    "\r\n"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ad4a7f633457edeca986db117c49439398b3575b7cfc8b89bb9541a12d05f016"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.7 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}